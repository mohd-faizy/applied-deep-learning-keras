{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Three-input models</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(ADVANCED DEEP LEARNING WITH KERAS)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Simple Model with 3 Inputs](#section-1)\n",
        "2. [Shared Layers with 3 Inputs](#section-2)\n",
        "3. [Fitting and Evaluating a 3-Input Model](#section-3)\n",
        "4. [Summarizing and Plotting Models](#section-4)\n",
        "5. [Stacking Models: Concepts and Data Preparation](#section-5)\n",
        "6. [3-Input Model with Pure Numeric Data](#section-6)\n",
        "7. [Conclusion](#section-7)\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. Simple Model with 3 Inputs</span><br>\n",
        "\n",
        "In advanced deep learning, models often require data from multiple sources. Keras allows us to define models with multiple input tensors. The simplest way to handle multiple inputs is to define them separately and then combine them using a `Concatenate` layer.\n",
        "\n",
        "### Architecture Overview\n",
        "1.  **Input Layers**: Three separate input tensors, each with a shape of `(1,)`.\n",
        "2.  **Concatenation**: Merges the three inputs into a single tensor.\n",
        "3.  **Output Layer**: A standard Dense layer to produce a prediction.\n",
        "\n",
        "### Implementation\n",
        "\n",
        "The following code demonstrates how to import the necessary layers and construct the model topology.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Concatenate, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 1. Define three separate input tensors\n",
        "# Each input expects a single number (shape=(1,))\n",
        "in_tensor_1 = Input(shape=(1,), name='input_1')\n",
        "in_tensor_2 = Input(shape=(1,), name='input_2')\n",
        "in_tensor_3 = Input(shape=(1,), name='input_3')\n",
        "\n",
        "# 2. Concatenate the inputs\n",
        "# The list contains the tensors we want to merge\n",
        "out_tensor = Concatenate()([in_tensor_1, in_tensor_2, in_tensor_3])\n",
        "\n",
        "# 3. Create the output layer\n",
        "# We use a Dense layer with 1 unit for a single regression output\n",
        "output_tensor = Dense(1, name='output_layer')(out_tensor)\n",
        "\n",
        "# 4. Instantiate the Model\n",
        "# We pass a list of inputs and the single output\n",
        "model = Model([in_tensor_1, in_tensor_2, in_tensor_3], output_tensor)\n",
        "\n",
        "print(\"Model created successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> When initializing the <code>Model</code> class, the <code>inputs</code> argument must be a list if there is more than one input tensor: <code>[input_1, input_2, input_3]</code>. </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. Shared Layers with 3 Inputs</span><br>\n",
        "\n",
        "Shared layers allow different inputs to pass through the exact same layer instance. This means the weights are shared (and trained jointly) across those inputs. This is useful when inputs represent the same type of entity (e.g., two different teams in a game).\n",
        "\n",
        "### Architecture Logic\n",
        "1.  Define a **single** `Dense` layer instance.\n",
        "2.  Apply this instance to multiple input tensors.\n",
        "3.  Concatenate the results with any other inputs.\n",
        "\n",
        "### Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a shared layer (instantiated once)\n",
        "shared_layer = Dense(1, name='shared_layer')\n",
        "\n",
        "# Apply the shared layer to the first input\n",
        "shared_tensor_1 = shared_layer(in_tensor_1)\n",
        "\n",
        "# Apply the SAME shared layer to the second input\n",
        "# (Note: In a real scenario, you might apply this to in_tensor_2, \n",
        "# but here we follow the pattern of applying operations to inputs)\n",
        "shared_tensor_2 = shared_layer(in_tensor_2)\n",
        "\n",
        "# Concatenate the shared outputs with the third raw input\n",
        "out_tensor_shared = Concatenate()([shared_tensor_1, shared_tensor_2, in_tensor_3])\n",
        "\n",
        "# Final output layer\n",
        "output_tensor_shared = Dense(1)(out_tensor_shared)\n",
        "\n",
        "# Create the model\n",
        "model_shared = Model([in_tensor_1, in_tensor_2, in_tensor_3], output_tensor_shared)\n",
        "\n",
        "print(\"Shared layer model created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. Fitting and Evaluating a 3-Input Model</span><br>\n",
        "\n",
        "When fitting a model with multiple inputs, the data passed to `.fit()` and `.evaluate()` must match the structure of the model inputs. If the model expects a list of 3 tensors, you must provide a list of 3 arrays (or a dictionary mapping names to arrays).\n",
        "\n",
        "### Data Preparation (Simulation)\n",
        "Since we do not have the original CSV, we will generate synthetic data to ensure the code below is executable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data for demonstration\n",
        "data_size = 1000\n",
        "train = pd.DataFrame({\n",
        "    'col1': np.random.random(data_size),\n",
        "    'col2': np.random.random(data_size),\n",
        "    'col3': np.random.random(data_size),\n",
        "    'target': np.random.random(data_size)\n",
        "})\n",
        "\n",
        "test = pd.DataFrame({\n",
        "    'col1': np.random.random(200),\n",
        "    'col2': np.random.random(200),\n",
        "    'col3': np.random.random(200),\n",
        "    'target': np.random.random(200)\n",
        "})\n",
        "\n",
        "print(\"Synthetic data generated.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Fitting the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(loss='mae', optimizer='adam')\n",
        "\n",
        "# Fit the model\n",
        "# Notice the input is a LIST of arrays: [col1, col2, col3]\n",
        "model.fit(\n",
        "    [train['col1'], train['col2'], train['col3']],\n",
        "    train['target'],\n",
        "    epochs=5,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Evaluating the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate using the same list structure for inputs\n",
        "evaluation = model.evaluate(\n",
        "    [test['col1'], test['col2'], test['col3']],\n",
        "    test['target']\n",
        ")\n",
        "\n",
        "print(f\"Test Loss (MAE): {evaluation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Summarizing and Plotting Models</span><br>\n",
        "\n",
        "Understanding the topology of complex models is crucial. Keras provides `model.summary()` for a text-based overview and `plot_model` for a visual graph.\n",
        "\n",
        "### Understanding `model.summary()`\n",
        "\n",
        "The summary provides a table of layers, their types, output shapes, parameter counts, and connections. Below is a reproduction of the summary table found in the document for a model with embeddings and flattening.\n",
        "\n",
        "**Model: \"model\"**\n",
        "\n",
        "| Layer (type) | Output Shape | Param # | Connected to |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **input_1** (InputLayer) | [(None, 1)] | 0 | [] |\n",
        "| **embedding** (Embedding) | (None, 1, 1) | 10887 | ['input_1[0][0]'] |\n",
        "| **flatten** (Flatten) | (None, 1) | 0 | ['embedding[0][0]'] |\n",
        "| **input_2** (InputLayer) | [(None, 1)] | 0 | [] |\n",
        "| **input_3** (InputLayer) | [(None, 1)] | 0 | [] |\n",
        "| **concatenate** (Concatenate) | (None, 3) | 0 | ['flatten[0][0]', 'input_2[0][0]', 'input_3[0][0]'] |\n",
        "| **dense** (Dense) | (None, 1) | 4 | ['concatenate[0][0]'] |\n",
        "\n",
        "*   **Total params:** 10,891\n",
        "*   **Trainable params:** 10,891\n",
        "*   **Non-trainable params:** 0\n",
        "\n",
        "### Generating a Summary in Python\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the summary of our currently active model\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Plotting the Model\n",
        "To visualize the directed acyclic graph (DAG) of the model layers, we use `plot_model`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Note: This requires pydot and graphviz installed in the environment\n",
        "try:\n",
        "    plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "    print(\"Model plot saved as 'model_plot.png'\")\n",
        "    \n",
        "    # To display in notebook (if image was generated)\n",
        "    # from IPython.display import Image\n",
        "    # display(Image('model_plot.png'))\n",
        "except ImportError:\n",
        "    print(\"Graphviz or pydot not installed. Skipping visualization generation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Stacking Models: Concepts and Data Preparation</span><br>\n",
        "\n",
        "Stacking involves using the predictions of one model as input features for another model. In this example, we use a model trained on **Regular Season** data to make predictions that enrich the dataset for a **Tournament** model.\n",
        "\n",
        "### Data Loading (Simulated)\n",
        "We will simulate the `games_season.csv` and `games_tourney.csv` datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulating the datasets described in the PDF\n",
        "games_season = pd.DataFrame({\n",
        "    'team_1': np.random.randint(1, 100, 5),\n",
        "    'team_2': np.random.randint(1, 100, 5),\n",
        "    'home': [0, 1, 1, 1, 1],\n",
        "    'score_diff': [17, 7, 7, 16, 12]\n",
        "})\n",
        "\n",
        "games_tourney = pd.DataFrame({\n",
        "    'team_1': [288, 5929, 9884, 73, 3920],\n",
        "    'team_2': [73, 73, 73, 288, 410],\n",
        "    'home': [0, 0, 0, 0, 0],\n",
        "    'seed_diff': [-3, 4, 5, 3, 1],\n",
        "    'score_diff': [-9, 6, -4, 9, -9]\n",
        "})\n",
        "\n",
        "print(\"Season Data Head:\")\n",
        "print(games_season.head())\n",
        "print(\"\\nTournament Data Head:\")\n",
        "print(games_tourney.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Enriching Tournament Data with Predictions\n",
        "We assume `regular_season_model` exists (we will use our `model` from Section 1 as a placeholder). We predict outcomes for the tournament games based on team inputs and add this prediction as a new column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare input data for prediction\n",
        "# Assuming the model expects 3 inputs: team_1, team_2, home\n",
        "in_data_1 = games_tourney['team_1']\n",
        "in_data_2 = games_tourney['team_2']\n",
        "in_data_3 = games_tourney['home']\n",
        "\n",
        "# Make predictions using the pre-trained model\n",
        "# (Using 'model' from Section 1 as the surrogate for 'regular_season_model')\n",
        "pred = model.predict([in_data_1, in_data_2, in_data_3])\n",
        "\n",
        "# Add predictions to the dataframe\n",
        "games_tourney['pred'] = pred\n",
        "\n",
        "print(\"\\nEnriched Tournament Data:\")\n",
        "print(games_tourney.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> Stacking allows you to leverage the strengths of different models trained on different domains (e.g., long-term season trends vs. high-stakes tournament dynamics). </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. 3-Input Model with Pure Numeric Data</span><br>\n",
        "\n",
        "Once the data is enriched with predictions, we might have a dataset that is purely numeric. In this case, we don't necessarily need 3 separate input layers if we treat the features as a single matrix. However, the PDF demonstrates a specific architecture using a single input tensor of shape `(3,)`.\n",
        "\n",
        "### Data Selection\n",
        "We select the relevant numeric columns: `home`, `seed_diff`, and the newly created `pred`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the data to be used\n",
        "print(games_tourney[['home', 'seed_diff', 'pred']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Defining the Numeric Model\n",
        "Instead of 3 inputs of shape `(1,)`, we use 1 input of shape `(3,)`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Define Input\n",
        "# Shape is (3,) because we have 3 columns: home, seed_diff, pred\n",
        "in_tensor_numeric = Input(shape=(3,))\n",
        "\n",
        "# 2. Define Output\n",
        "out_tensor_numeric = Dense(1)(in_tensor_numeric)\n",
        "\n",
        "# 3. Create Model\n",
        "model_numeric = Model(in_tensor_numeric, out_tensor_numeric)\n",
        "\n",
        "# 4. Compile\n",
        "model_numeric.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "print(\"Numeric model compiled.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Fitting the Numeric Model\n",
        "We prepare the training and testing sets. Note that `train_X` is now a single DataFrame (or matrix), not a list of columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare Training Data (Using games_tourney as train for demo)\n",
        "train_X = games_tourney[['home', 'seed_diff', 'pred']]\n",
        "train_y = games_tourney['score_diff']\n",
        "\n",
        "# Fit the model\n",
        "model_numeric.fit(\n",
        "    train_X, \n",
        "    train_y, \n",
        "    epochs=10, \n",
        "    validation_split=0.10,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Evaluating the Numeric Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare Test Data (Using same data for demo purposes)\n",
        "test_X = games_tourney[['home', 'seed_diff', 'pred']]\n",
        "test_y = games_tourney['score_diff']\n",
        "\n",
        "# Evaluate\n",
        "result = model_numeric.evaluate(test_X, test_y)\n",
        "print(f\"Evaluation Result (MAE): {result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 7. Conclusion</span><br>\n",
        "\n",
        "In this notebook, we explored advanced Keras techniques for handling complex data structures and model architectures.\n",
        "\n",
        "**Key Takeaways:**\n",
        "1.  **Multiple Inputs**: We can create models that accept multiple tensors using `Input` layers and merge them using `Concatenate`. This is essential for multimodal learning or combining distinct data sources.\n",
        "2.  **Shared Layers**: Layers can be reused across different inputs to share weights, which is highly effective for comparing similar entities (like sports teams).\n",
        "3.  **Model Inspection**: Tools like `model.summary()` and `plot_model` are vital for debugging and verifying the topology of complex graphs.\n",
        "4.  **Stacking**: We demonstrated how to use the output of one model (Regular Season) as a feature input for a second model (Tournament), a powerful technique in ensemble learning.\n",
        "\n",
        "**Next Steps:**\n",
        "*   Experiment with different merging layers (e.g., `Add`, `Multiply`) instead of just `Concatenate`.\n",
        "*   Apply these techniques to different domains, such as combining image data (CNN) with metadata (Dense layers).\n",
        "*   Explore 3-output models to predict multiple targets simultaneously.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}