{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Data Pre-processing & RNNs for Language Modeling</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Text Classification, Transfer Learning, and Model Assessment with Keras)</span></div>\n",
        "\n",
        "# Table of Contents\n",
        "\n",
        "1. [Text Classification Fundamentals](#section-1)\n",
        "2. [Transitioning from Binary to Multi-class Classification](#section-2)\n",
        "3. [Transfer Learning for Language Models](#section-3)\n",
        "4. [Multi-class Classification Models with Keras](#section-4)\n",
        "5. [Assessing Model Performance](#section-5)\n",
        "6. [Conclusion](#section-6)\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. Text Classification Fundamentals</span><br>\n",
        "\n",
        "Text classification is a fundamental task in Natural Language Processing (NLP). It involves assigning predefined categories to text documents.\n",
        "\n",
        "### Applications of Text Classification\n",
        "Text classification is ubiquitous in industry and research. Common applications include:\n",
        "*   **Automatic news classification**: Categorizing articles into topics like Sports, Finance, or Politics.\n",
        "*   **Document classification for businesses**: Sorting invoices, contracts, or internal memos.\n",
        "*   **Queue segmentation for customer support**: Routing support tickets to the correct department (e.g., Technical Support vs. Billing) based on the text content.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. Transitioning from Binary to Multi-class Classification</span><br>\n",
        "\n",
        "When moving from binary classification (2 classes, e.g., Positive/Negative) to multi-class classification (3+ classes), several architectural changes are required in the neural network.\n",
        "\n",
        "### Key Changes\n",
        "| Component | Binary Classification | Multi-class Classification |\n",
        "| :--- | :--- | :--- |\n",
        "| **Output Variable ($y$)** | Scalar (0 or 1) | One-hot encoded vector |\n",
        "| **Output Units** | 1 Unit | `num_classes` Units |\n",
        "| **Activation Function** | Sigmoid | Softmax |\n",
        "| **Loss Function** | Binary Cross-entropy | Categorical Cross-entropy |\n",
        "\n",
        "### 1. Shape of the Output Variable ($y$)\n",
        "In multi-class settings, classes are often represented using **One-hot encoding**.\n",
        "*   If `num_classes = 3`, a label is a vector of length 3.\n",
        "*   Example: Class 1 might be `[0, 1, 0]`.\n",
        "*   The shape of $y$ becomes $(N, \\text{num\\_classes})$.\n",
        "\n",
        "### 2. Output Layer Architecture\n",
        "The number of units in the final dense layer must match the number of classes.\n",
        "\n",
        "**Original Code (Conceptual Keras):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: num_classes = 3\n",
        "# y[0] = [0, 1, 0]\n",
        "# y.shape = (N, num_classes)\n",
        "\n",
        "# Output layer\n",
        "# model.add(Dense(num_classes))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 3. Activation Function: Softmax\n",
        "Instead of `sigmoid`, we use `softmax`. Softmax ensures that the output values represent the probability distribution across all classes (summing to 1).\n",
        "\n",
        "**Original Code:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Output layer\n",
        "# model.add(Dense(num_classes, activation=\"softmax\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 4. Loss Function: Categorical Cross-entropy\n",
        "We switch the loss function to handle multi-class targets.\n",
        "\n",
        "**Original Code:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "# model.compile(loss='categorical_crossentropy')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Practical Example: Preparing Categories\n",
        "We can use Pandas to convert string labels into numerical codes, and Keras to convert those codes into one-hot vectors.\n",
        "\n",
        "**Original Code (Pandas Series):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "y = [\"sports\", \"economy\", \"data_science\", \"sports\", \"finance\"]\n",
        "# Transform to pandas series object\n",
        "y_series = pd.Series(y, dtype=\"category\")\n",
        "# Print the category codes\n",
        "print(y_series.cat.codes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Enhanced Code (Runnable Pre-processing):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Convert text labels to numeric codes\n",
        "y_labels = [\"sports\", \"economy\", \"data_science\", \"sports\", \"finance\"]\n",
        "y_series = pd.Series(y_labels, dtype=\"category\")\n",
        "\n",
        "print(\"Category Codes:\")\n",
        "print(y_series.cat.codes)\n",
        "\n",
        "# 2. Convert numeric codes to One-Hot Encoding\n",
        "# Let's assume we have numeric labels [0, 1, 2]\n",
        "y_numeric = np.array([0, 1, 2])\n",
        "\n",
        "# Change to categorical\n",
        "y_prep = to_categorical(y_numeric)\n",
        "\n",
        "print(\"\\nOne-Hot Encoded Output:\")\n",
        "print(y_prep)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> In a multi-class geometric space (e.g., $\\mathbb{R}^3$), one-hot encoding ensures that all classes are equidistant from each other (Distance = $\\sqrt{2}$), preventing the model from assuming an ordinal relationship (e.g., that class 2 is \"greater\" than class 1). </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. Transfer Learning for Language Models</span><br>\n",
        "\n",
        "Transfer learning involves taking a model trained on a massive dataset and applying its knowledge to a new, related task.\n",
        "\n",
        "### The Idea Behind Transfer Learning\n",
        "*   **Better Initialization**: Start with weights that are better than random.\n",
        "*   **Big Datasets**: Leverage models trained on Wikipedia, Google News, etc.\n",
        "*   **Open Source**: Utilize pre-trained models available in the data science community.\n",
        "\n",
        "### Available Architectures\n",
        "\n",
        "| Architecture | Description | Input/Output Example |\n",
        "| :--- | :--- | :--- |\n",
        "| **Word2Vec** | Maps words to vectors. Two types: **CBOW** (Predict target from context) and **Skip-gram** (Predict context from target). | *Context*: \"I really loved this movie\" <br> **CBOW**: $X=$[I, really, this, movie], $y=$ loved |\n",
        "| **FastText** | Uses words and **n-grams of characters**. Better for rare words or typos. | $X=$ [I, rea, eal, all, lly, really...], $y=$ loved |\n",
        "| **ELMo** | **Embeddings from Language Models**. Uses deep bidirectional language models (biLM). Embeddings change based on context. | $X=$ [I, really, loved, this], $y=$ movie |\n",
        "\n",
        "### Implementation: Word2Vec with Gensim\n",
        "Word2Vec and FastText are available in the `gensim` package.\n",
        "\n",
        "**Original Code (Word2Vec):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "# Note: In the PDF, variables like tokenized_corpus are assumed to exist.\n",
        "# w2v_model = word2vec.Word2Vec(tokenized_corpus, size=embedding_dim, \n",
        "#                               window=neighbor_words_num, iter=100)\n",
        "\n",
        "# Get top 3 similar words to \"captain\"\n",
        "# w2v_model.wv.most_similar([\"captain\"], topn=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Enhanced Code (Runnable with Gensim 4.x):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# 1. Create a dummy corpus\n",
        "corpus = [\n",
        "    \"I really loved this movie\",\n",
        "    \"The captain wore sweatpants\",\n",
        "    \"Kirk is the captain of the ship\",\n",
        "    \"Larry is a friend of the captain\"\n",
        "]\n",
        "tokenized_corpus = [simple_preprocess(sentence) for sentence in corpus]\n",
        "\n",
        "# 2. Train Word2Vec Model\n",
        "# Note: 'size' is renamed to 'vector_size' and 'iter' to 'epochs' in newer Gensim\n",
        "w2v_model = Word2Vec(sentences=tokenized_corpus, \n",
        "                     vector_size=100, \n",
        "                     window=5, \n",
        "                     min_count=1, \n",
        "                     epochs=100)\n",
        "\n",
        "# 3. Get similar words (if 'captain' exists in vocab)\n",
        "if 'captain' in w2v_model.wv:\n",
        "    print(\"Similar to 'captain':\")\n",
        "    print(w2v_model.wv.most_similar([\"captain\"], topn=3))\n",
        "else:\n",
        "    print(\"'captain' not in vocabulary due to small corpus size.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Implementation: FastText with Gensim\n",
        "\n",
        "**Original Code (FastText):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import fasttext\n",
        "\n",
        "# Instantiate the model\n",
        "# ft_model = fasttext.FastText(size=embedding_dim, window=neighbor_words_num)\n",
        "\n",
        "# Build vocabulary\n",
        "# ft_model.build_vocab(sentences=tokenized_corpus)\n",
        "\n",
        "# Train the model\n",
        "# ft_model.train(sentences=tokenized_corpus,\n",
        "#                total_examples=len(tokenized_corpus),\n",
        "#                epochs=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Multi-class Classification Models with Keras</span><br>\n",
        "\n",
        "We will now build a complete pipeline for multi-class classification using the **20 News Groups** dataset.\n",
        "\n",
        "### Model Architecture Comparison\n",
        "\n",
        "**1. Binary Classification (Review)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 128))\n",
        "model.add(LSTM(128, dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid')) # Single unit, sigmoid\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**2. Multi-class Classification**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = 20 # Example for 20 News Groups\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 128))\n",
        "model.add(LSTM(128, dropout=0.2))\n",
        "# Output layer has `num_classes` units and uses `softmax`\n",
        "model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "# Compile with categorical crossentropy\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### The Dataset: 20 News Groups\n",
        "We use `sklearn` to fetch the data.\n",
        "\n",
        "**Attributes:**\n",
        "*   `news_train.DESCR`: Documentation.\n",
        "*   `news_train.data`: The raw text data.\n",
        "*   `news_train.filenames`: Paths on disk.\n",
        "*   `news_train.target`: Numerical index of classes.\n",
        "*   `news_train.target_names`: Human-readable names.\n",
        "\n",
        "**Original Code (Loading Data):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Download train and test sets\n",
        "news_train = fetch_20newsgroups(subset='train')\n",
        "news_test = fetch_20newsgroups(subset='test')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Pre-processing Pipeline\n",
        "We must tokenize the text, pad the sequences to a fixed length, and one-hot encode the targets.\n",
        "\n",
        "**Original Code (Preprocessing):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Create and fit the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(news_train.data)\n",
        "\n",
        "# Create the (X, Y) variables\n",
        "X_train = tokenizer.texts_to_sequences(news_train.data)\n",
        "X_train = pad_sequences(X_train, maxlen=400)\n",
        "Y_train = to_categorical(news_train.target)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Training the Model\n",
        "**Original Code (Training):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.fit(X_train, Y_train, batch_size=64, epochs=100)\n",
        "\n",
        "# Evaluate on test data\n",
        "# Note: X_test and Y_test would need similar preprocessing\n",
        "# model.evaluate(X_test, Y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Assessing Model Performance</span><br>\n",
        "\n",
        "Accuracy alone is often insufficient, especially with imbalanced datasets.\n",
        "\n",
        "### Why Accuracy is Not Enough\n",
        "If you have a 20-class task and achieve 80% accuracy:\n",
        "*   Can it classify *all* classes correctly?\n",
        "*   Is the accuracy the same for each class?\n",
        "*   Is the model overfitting on the majority class?\n",
        "\n",
        "### The Confusion Matrix\n",
        "A confusion matrix shows the True classes vs. the Predicted classes.\n",
        "\n",
        "| True Class \\ Predicted | sci.space | alt.atheism | soc.religion.christian |\n",
        "| :--- | :---: | :---: | :---: |\n",
        "| **sci.space** | **76** | 2 | 0 |\n",
        "| **alt.atheism** | 7 | **1** | 2 |\n",
        "| **soc.religion.christian** | 9 | 0 | **3** |\n",
        "\n",
        "### Performance Metrics Formulas\n",
        "\n",
        "**1. Precision**\n",
        "$$ \\text{Precision}_{\\text{class}} = \\frac{\\text{Correct}_{\\text{class}}}{\\text{Predicted}_{\\text{class}}} $$\n",
        "\n",
        "*   Example (sci.space): $\\frac{76}{76 + 7 + 9} = \\frac{76}{92} = 0.83$\n",
        "\n",
        "**2. Recall**\n",
        "$$ \\text{Recall}_{\\text{class}} = \\frac{\\text{Correct}_{\\text{class}}}{N_{\\text{class}}} $$\n",
        "\n",
        "*   Example (sci.space): $\\frac{76}{76 + 2 + 0} = \\frac{76}{78} = 0.97$\n",
        "\n",
        "**3. F1-Score**\n",
        "$$ \\text{F1 score} = 2 * \\frac{\\text{precision}_{\\text{class}} * \\text{recall}_{\\text{class}}}{\\text{precision}_{\\text{class}} + \\text{recall}_{\\text{class}}} $$\n",
        "\n",
        "*   Example (sci.space): $2 * \\frac{0.83 * 0.97}{0.83 + 0.97} = 0.89$\n",
        "\n",
        "### Implementation with Sklearn\n",
        "We can calculate these metrics using `sklearn.metrics`.\n",
        "\n",
        "**Original Code (Confusion Matrix):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Build the confusion matrix\n",
        "# confusion_matrix(y_true, y_pred)\n",
        "# Output:\n",
        "# array([[76, 2, 0],\n",
        "#        [ 7, 1, 2],\n",
        "#        [ 9, 0, 3]], dtype=int64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Enhanced Code (Full Metrics Calculation):**\n",
        "This code replicates the exact numbers found in the PDF presentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "\n",
        "# Recreating the data from the PDF Confusion Matrix\n",
        "# Matrix structure:\n",
        "# [[76, 2, 0],  <- True: sci.space\n",
        "#  [ 7, 1, 2],  <- True: alt.atheism\n",
        "#  [ 9, 0, 3]]  <- True: soc.religion.christian\n",
        "\n",
        "# We construct y_true and y_pred to match this matrix\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Class 0 (sci.space): 76 correct, 2 pred as 1, 0 pred as 2\n",
        "y_true.extend([0]*78)\n",
        "y_pred.extend([0]*76 + [1]*2 + [2]*0)\n",
        "\n",
        "# Class 1 (alt.atheism): 7 pred as 0, 1 correct, 2 pred as 2\n",
        "y_true.extend([1]*10)\n",
        "y_pred.extend([0]*7 + [1]*1 + [2]*2)\n",
        "\n",
        "# Class 2 (soc.religion.christian): 9 pred as 0, 0 pred as 1, 3 correct\n",
        "y_true.extend([2]*12)\n",
        "y_pred.extend([0]*9 + [1]*0 + [2]*3)\n",
        "\n",
        "# 1. Accuracy\n",
        "print(f\"Accuracy: {accuracy_score(y_true, y_pred):.2f}\")\n",
        "\n",
        "# 2. Precision, Recall, F1 (average=None gives per-class scores)\n",
        "print(\"\\nPrecision:\", precision_score(y_true, y_pred, average=None))\n",
        "print(\"Recall:   \", recall_score(y_true, y_pred, average=None))\n",
        "print(\"F1 Score: \", f1_score(y_true, y_pred, average=None))\n",
        "\n",
        "# 3. Classification Report\n",
        "lab_names = ['sci.space', 'alt.atheism', 'soc.religion.christian']\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=lab_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> The <code>classification_report</code> function is a powerful tool that calculates precision, recall, f1-score, and support (number of samples) for every class in a single function call. </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. Conclusion</span><br>\n",
        "\n",
        "In this notebook, we explored the advanced steps required to build robust Language Models using Recurrent Neural Networks (RNNs) and Keras.\n",
        "\n",
        "**Key Takeaways:**\n",
        "1.  **Multi-class Classification**: Requires changing the output layer to `softmax`, the loss to `categorical_crossentropy`, and the target variable to a one-hot encoded format.\n",
        "2.  **Transfer Learning**: Utilizing pre-trained embeddings like Word2Vec, FastText, or ELMo can significantly boost performance by initializing the model with learned semantic relationships.\n",
        "3.  **Performance Assessment**: Accuracy is often misleading in multi-class problems. Using a **Confusion Matrix** along with **Precision**, **Recall**, and **F1-Score** provides a granular view of how well the model performs for each specific category.\n",
        "\n",
        "**Next Steps:**\n",
        "*   Experiment with different pre-trained embeddings (e.g., GloVe or BERT).\n",
        "*   Apply these techniques to your own text datasets.\n",
        "*   Tune hyperparameters (dropout rates, LSTM units) to improve the F1-scores of underperforming classes.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}