{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Vanishing and Exploding Gradients in RNNs</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Advanced Language Modeling with Keras, GRUs, LSTMs, and Embeddings)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [The Problem: Vanishing and Exploding Gradients](#section-1)\n",
        "2. [Solutions to Gradient Problems](#section-2)\n",
        "3. [GRU and LSTM Cells Architecture](#section-3)\n",
        "4. [Implementing GRU and LSTM in Keras](#section-4)\n",
        "5. [The Embedding Layer](#section-5)\n",
        "6. [Transfer Learning with GloVe](#section-6)\n",
        "7. [Sentiment Classification Revisited](#section-7)\n",
        "8. [Avoiding Overfitting in RNNs](#section-8)\n",
        "9. [Advanced Architectures: CNNs and Complex Models](#section-9)\n",
        "10. [Conclusion](#section-10)\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. The Problem: Vanishing and Exploding Gradients</span><br>\n",
        "\n",
        "### Forward Propagation in RNNs\n",
        "Recurrent Neural Networks (RNNs) process sequences by maintaining a hidden state that evolves over time. During **Forward Propagation**, information flows from the start of the sequence to the end.\n",
        "\n",
        "The hidden state $a_t$ at time step $t$ is calculated as a function of the weight matrix $W_a$, the previous hidden state $a_{t-1}$, and the current input $x_t$.\n",
        "\n",
        "$$ a_t = f(W_a, a_{t-1}, x_t) $$\n",
        "\n",
        "Because this is recursive, $a_t$ depends on $a_{t-1}$, which depends on $a_{t-2}$, and so on. Expanding this:\n",
        "\n",
        "$$ a_2 = f(W_a, a_1, x_2) = f(W_a, f(W_a, a_0, x_1), x_2) $$\n",
        "\n",
        "### Backpropagation Through Time (BPTT)\n",
        "To train the network, we use **Backpropagation Through Time**. We calculate the gradient of the loss function with respect to the weights to update them.\n",
        "\n",
        "However, because of the recursive nature, the derivative of the state $a_t$ with respect to the weights $W_a$ involves the chain rule across all previous time steps.\n",
        "\n",
        "$$ \\frac{\\partial a_t}{\\partial W_a} \\propto (W_a)^{t-1} g(X) $$\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> The term $(W_a)^{t-1}$ is the culprit. If the weights are small (< 1), the gradient shrinks exponentially to 0 (Vanishing). If they are large (> 1), it grows exponentially to infinity (Exploding). </div>\n",
        "\n",
        "### Consequences\n",
        "1.  **Vanishing Gradients**: The model stops learning from earlier time steps. It \"forgets\" long-term dependencies.\n",
        "2.  **Exploding Gradients**: The weights update massively, causing the loss to oscillate or diverge (NaN).\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. Solutions to Gradient Problems</span><br>\n",
        "\n",
        "There are several standard techniques to mitigate these issues.\n",
        "\n",
        "### Solutions for Exploding Gradients\n",
        "*   **Gradient Clipping / Scaling**: Cap the gradients at a maximum threshold before updating weights.\n",
        "\n",
        "### Solutions for Vanishing Gradients\n",
        "*   **Better Initialization**: Initialize the matrix $W$ carefully (e.g., Xavier or He initialization).\n",
        "*   **Regularization**: Use techniques to constrain model complexity.\n",
        "*   **Activation Functions**: Use **ReLU** instead of tanh or sigmoid (which saturate and kill gradients).\n",
        "*   **Architecture Changes**: Use **LSTM** (Long Short-Term Memory) or **GRU** (Gated Recurrent Unit) cells.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Example of Gradient Clipping in the Optimizer\n",
        "# 'clipnorm' scales gradients so their norm is at most 1.0\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, clipnorm=1.0)\n",
        "\n",
        "# Example of using ReLU activation to help with vanishing gradients\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=64, activation='relu', input_shape=(None, 10)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "print(\"Model compiled with Gradient Clipping and ReLU activation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. GRU and LSTM Cells Architecture</span><br>\n",
        "\n",
        "Standard `SimpleRNN` cells multiply the weight matrix at every step, leading to the vanishing gradient problem. **GRU** and **LSTM** cells introduce **gates** that control the flow of information, allowing the network to decide what to keep and what to forget.\n",
        "\n",
        "### 1. SimpleRNN Cell (The Baseline)\n",
        "*   **Logic**: $a_t = f(W_a, a_{t-1}, x_t)$\n",
        "*   **Issue**: Direct multiplication leads to instability.\n",
        "\n",
        "### 2. GRU Cell (Gated Recurrent Unit)\n",
        "Simplified version of LSTM with two gates.\n",
        "*   **Update Gate ($g_u$)**: Decides how much of the past information to pass along to the future.\n",
        "*   **Candidate Memory ($\\tilde{a}_t$)**: A temporary new memory value.\n",
        "*   **Final Memory ($a_t$)**: A linear interpolation between the past state and the new candidate.\n",
        "\n",
        "$$ a_t = h(g_u, \\tilde{a}_t, a_{t-1}) $$\n",
        "\n",
        "### 3. LSTM Cell (Long Short-Term Memory)\n",
        "More complex, with three distinct gates and a separate cell state ($c_t$).\n",
        "*   **Forget Gate ($g_f$)**: Decides what to remove from the cell state.\n",
        "*   **Update Gate ($g_u$)**: Decides what new information to store in the cell state.\n",
        "*   **Output Gate ($g_o$)**: Decides what parts of the cell state to output as the hidden state $a_t$.\n",
        "\n",
        "**Why they solve Vanishing Gradients:**\n",
        "Because the cell state $c_t$ is updated via addition (not just multiplication), gradients can flow through the network for many time steps without vanishing.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Implementing GRU and LSTM in Keras</span><br>\n",
        "\n",
        "Keras provides built-in layers for these architectures. They are drop-in replacements for `SimpleRNN`.\n",
        "\n",
        "### Code Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import GRU, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Initialize model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a GRU layer\n",
        "# units=128: Dimension of the hidden state\n",
        "# return_sequences=True: Output the full sequence (needed if stacking RNN layers)\n",
        "model.add(GRU(units=128, return_sequences=True, input_shape=(None, 50), name='GRU_layer'))\n",
        "\n",
        "# Add an LSTM layer\n",
        "# return_sequences=False: Only output the final state (typical for the last RNN layer before classification)\n",
        "model.add(LSTM(units=64, return_sequences=False, name='LSTM_layer'))\n",
        "\n",
        "# Summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> Use <code>return_sequences=True</code> when you want to stack multiple RNN layers. The last RNN layer usually has <code>return_sequences=False</code> to feed into a Dense layer. </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. The Embedding Layer</span><br>\n",
        "\n",
        "When dealing with text, we need to convert words into numbers.\n",
        "\n",
        "### One-Hot Encoding vs. Embeddings\n",
        "\n",
        "| Feature | One-Hot Encoding | Word Embeddings |\n",
        "| :--- | :--- | :--- |\n",
        "| **Dimension** | High (Vocabulary Size, e.g., 100,000) | Low (e.g., 300) |\n",
        "| **Sparsity** | Sparse (Mostly zeros) | Dense (Real numbers) |\n",
        "| **Semantics** | No semantic relationship | Captures meaning (King - Man + Woman = Queen) |\n",
        "| **Training** | Fixed | Learned during training or pre-trained |\n",
        "\n",
        "### Using the Embedding Layer in Keras\n",
        "The `Embedding` layer is usually the first layer of a language model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Parameters:\n",
        "# input_dim: Size of the vocabulary (e.g., 100,000 words)\n",
        "# output_dim: Dimension of the dense embedding vector (e.g., 300)\n",
        "# input_length: Length of input sequences (e.g., 120 words)\n",
        "model.add(Embedding(input_dim=100000, \n",
        "                    output_dim=300, \n",
        "                    trainable=True,\n",
        "                    embeddings_initializer=None,\n",
        "                    input_length=120))\n",
        "\n",
        "print(\"Embedding layer added.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Disadvantage**: Embeddings add many parameters to the model, which can increase training time.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. Transfer Learning with GloVe</span><br>\n",
        "\n",
        "Instead of learning embeddings from scratch, we can use **Transfer Learning**. We use pre-trained vectors like **GloVe**, **word2vec**, or **BERT**.\n",
        "\n",
        "### 1. Loading GloVe Vectors\n",
        "We parse a text file where each line contains a word followed by its vector coefficients.\n",
        "\n",
        "*(Note: The code below includes a mock file generator so you can run it immediately without downloading the 1GB GloVe file).*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- MOCK DATA GENERATION (For demonstration purposes) ---\n",
        "# In a real scenario, you would download 'glove.6B.300d.txt'\n",
        "mock_glove_content = \"\"\"the 0.418 0.24968 -0.41242 0.1217 0.34527\n",
        "cat 0.013441 0.23682 -0.16899 0.40951 0.63812\n",
        "dog 0.1529 0.30091 -0.29451 0.22737 0.2962\n",
        "\"\"\"\n",
        "with open(\"glove.mock.5d.txt\", \"w\") as f:\n",
        "    f.write(mock_glove_content)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def get_glove_vectors(filename):\n",
        "    \"\"\"\n",
        "    Parses the GloVe text file into a dictionary.\n",
        "    \"\"\"\n",
        "    glove_vector_dict = {}\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = values[1:]\n",
        "            glove_vector_dict[word] = np.asarray(coefs, dtype='float32')\n",
        "            \n",
        "    return glove_vector_dict\n",
        "\n",
        "# Load the vectors (using our mock file with 5 dimensions for simplicity)\n",
        "glove_dict = get_glove_vectors(\"glove.mock.5d.txt\")\n",
        "print(f\"Loaded {len(glove_dict)} word vectors.\")\n",
        "print(f\"Vector for 'cat': {glove_dict['cat']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 2. Filtering GloVe for Specific Task\n",
        "We create an embedding matrix that matches our specific vocabulary index.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_glove(vocabulary_dict, glove_dict, wordvec_dim):\n",
        "    \"\"\"\n",
        "    Creates an embedding matrix for the specific vocabulary.\n",
        "    Rows correspond to the index in vocabulary_dict.\n",
        "    \"\"\"\n",
        "    # +1 for the 0 index (padding)\n",
        "    embedding_matrix = np.zeros((len(vocabulary_dict) + 1, wordvec_dim))\n",
        "    \n",
        "    for word, i in vocabulary_dict.items():\n",
        "        embedding_vector = glove_dict.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Words found in GloVe get their vector\n",
        "            # Words not found remain all-zeros\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    return embedding_matrix\n",
        "\n",
        "# Example usage\n",
        "vocab = {'the': 1, 'cat': 2, 'zebra': 3} # 'zebra' is not in our mock glove\n",
        "matrix = filter_glove(vocab, glove_dict, wordvec_dim=5)\n",
        "\n",
        "print(\"Embedding Matrix shape:\", matrix.shape)\n",
        "print(\"Row 2 (cat):\", matrix[2])\n",
        "print(\"Row 3 (zebra - missing):\", matrix[3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 3. Using Pre-trained Vectors in Keras\n",
        "We initialize the `Embedding` layer with `Constant` weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "vocabulary_size = len(vocab) + 1\n",
        "embedding_dim = 5\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocabulary_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    embeddings_initializer=Constant(matrix),\n",
        "                    trainable=False)) # Often set to False to keep pre-trained values\n",
        "\n",
        "print(\"Pre-trained embedding layer created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 7. Sentiment Classification Revisited</span><br>\n",
        "\n",
        "### The Baseline Problem\n",
        "A simple model might yield poor results (e.g., Accuracy ~0.5, Loss ~0.69).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Previous poor performing model example\n",
        "# model.add(SimpleRNN(units=16, input_shape=(None, 1)))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "# Result: [0.699, 0.495] -> Random guessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Strategies for Improvement\n",
        "To improve performance on sentiment analysis tasks:\n",
        "1.  **Add an Embedding Layer**: Capture semantic meaning.\n",
        "2.  **Increase Depth**: Add more layers.\n",
        "3.  **Tune Parameters**: Adjust units, learning rate, etc.\n",
        "4.  **Increase Vocabulary**: Allow the model to recognize more words.\n",
        "5.  **Longer Sentences**: Increase `input_length` to capture more context.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 8. Avoiding Overfitting in RNNs</span><br>\n",
        "\n",
        "RNNs are prone to overfitting (memorizing the training data). We use **Dropout** to mitigate this.\n",
        "\n",
        "1.  **Standard Dropout**: Randomly sets inputs to 0.\n",
        "2.  **Recurrent Dropout**: Randomly drops connections *within* the recurrent units (memory cells).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1. Standard Dropout Layer\n",
        "# Removes 20% of input to add noise\n",
        "model.add(Dropout(rate=0.2, input_shape=(None, 50)))\n",
        "\n",
        "# 2. Dropout inside RNN layers\n",
        "# dropout=0.1: Dropout for input units\n",
        "# recurrent_dropout=0.1: Dropout for recurrent state\n",
        "model.add(LSTM(128, dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "print(\"Model with Dropout configuration created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 9. Advanced Architectures: CNNs and Complex Models</span><br>\n",
        "\n",
        "### 1. 1D Convolutional Layers (Conv1D)\n",
        "While typically used for images, Convolutions can be used in NLP to perform feature selection on embedding vectors. They are fast and effective.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=300, input_length=100))\n",
        "\n",
        "# Conv1D Layer\n",
        "# num_filters=32: Number of output filters\n",
        "# kernel_size=3: Window size (looks at 3 words at a time)\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "print(\"Conv1D layer added.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 2. A Complex \"State-of-the-Art\" Style Architecture\n",
        "Combining Embeddings, Dense layers, Dropout, LSTM, and GRU into a single powerful model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "vocabulary_size = 5000\n",
        "wordvec_dim = 300\n",
        "max_text_len = 100\n",
        "\n",
        "# Define the complex model\n",
        "model = Sequential()\n",
        "\n",
        "# 1. Embedding Layer (using pre-trained weights logic)\n",
        "# Note: In a real run, 'matrix' would be the full 300-dim GloVe matrix\n",
        "model.add(Embedding(input_dim=vocabulary_size, \n",
        "                    output_dim=wordvec_dim, \n",
        "                    trainable=True,\n",
        "                    # embeddings_initializer=Constant(matrix), # Uncomment if matrix is ready\n",
        "                    input_length=max_text_len, \n",
        "                    name=\"Embedding\"))\n",
        "\n",
        "# 2. Dense Layer for feature transformation\n",
        "model.add(Dense(wordvec_dim, activation='relu', name=\"Dense1\"))\n",
        "\n",
        "# 3. Dropout\n",
        "model.add(Dropout(rate=0.25))\n",
        "\n",
        "# 4. LSTM Layer (Returning sequences to feed into GRU)\n",
        "model.add(LSTM(64, return_sequences=True, dropout=0.15, name=\"LSTM\"))\n",
        "\n",
        "# 5. GRU Layer (Not returning sequences, final recurrent step)\n",
        "model.add(GRU(64, return_sequences=False, dropout=0.15, name=\"GRU\"))\n",
        "\n",
        "# 6. Dense Layers for classification\n",
        "model.add(Dense(64, name=\"Dense2\"))\n",
        "model.add(Dropout(rate=0.25))\n",
        "model.add(Dense(32, name=\"Dense3\"))\n",
        "\n",
        "# 7. Output Layer (Binary Classification)\n",
        "model.add(Dense(1, activation='sigmoid', name=\"Output\"))\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 10. Conclusion</span><br>\n",
        "\n",
        "In this notebook, we explored the challenges and solutions for training Recurrent Neural Networks for language modeling.\n",
        "\n",
        "**Key Takeaways:**\n",
        "1.  **Gradient Problems**: Standard RNNs suffer from vanishing gradients due to repeated matrix multiplication during Backpropagation Through Time (BPTT).\n",
        "2.  **Architectural Solutions**: **LSTM** and **GRU** cells utilize gating mechanisms (Update, Forget, Output) to preserve gradients over long sequences, effectively solving the vanishing gradient problem.\n",
        "3.  **Embeddings**: Moving from One-Hot encoding to **Word Embeddings** allows for dense, semantic representations of text.\n",
        "4.  **Transfer Learning**: Using pre-trained vectors like **GloVe** can significantly boost performance by leveraging external knowledge.\n",
        "5.  **Regularization**: **Dropout** and **Recurrent Dropout** are essential to prevent overfitting in complex RNN models.\n",
        "\n",
        "**Next Steps:**\n",
        "*   Experiment with hyperparameter tuning (number of units, dropout rates).\n",
        "*   Apply these architectures to real-world datasets like IMDB Sentiment Analysis or text generation.\n",
        "*   Explore Transformer architectures (BERT, GPT) which have largely superseded RNNs for very long sequences.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}