{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Sequence to Sequence Models</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Recurrent Neural Networks for Language Modeling with Keras)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction to Sequence Architectures](#section-1)\n",
        "2. [Text Generation: Concepts & Examples](#section-2)\n",
        "3. [The Text Generating Function](#section-3)\n",
        "4. [Probability Scaling (Temperature)](#section-4)\n",
        "5. [Text Generation Model Architecture](#section-5)\n",
        "6. [Neural Machine Translation (NMT) Overview](#section-6)\n",
        "7. [NMT: Encoder Architecture](#section-7)\n",
        "8. [NMT: Decoder Architecture](#section-8)\n",
        "9. [Data Preparation for NMT](#section-9)\n",
        "10. [Training and Evaluation](#section-10)\n",
        "11. [Course Wrap-up and RNN Pitfalls](#section-11)\n",
        "12. [Conclusion](#section-12)\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. Introduction to Sequence Architectures</span><br>\n",
        "\n",
        "Sequence-to-sequence models are a powerful class of Recurrent Neural Networks (RNNs) used to map input sequences to output sequences. Depending on the task, the relationship between input and output length can vary significantly.\n",
        "\n",
        "### Possible Architectures\n",
        "\n",
        "There are two primary architectural patterns discussed in this module:\n",
        "\n",
        "| Architecture Type | Description | Examples |\n",
        "| :--- | :--- | :--- |\n",
        "| **Many-to-One** | Many inputs mapped to a single output. | â€¢ Sentiment Analysis<br>â€¢ Classification |\n",
        "| **Many-to-Many** | Many inputs mapped to many outputs. | â€¢ Text Generation<br>â€¢ Neural Machine Translation (NMT) |\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> In this notebook, we focus on <b>Many-to-Many</b> architectures, specifically for Text Generation and Neural Machine Translation. </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. Text Generation: Concepts & Examples</span><br>\n",
        "\n",
        "Text generation involves training a model to predict the next token in a sequence. This can be used to mimic the style of a specific author or character.\n",
        "\n",
        "### Example: Mimicking Sheldon Cooper\n",
        "Using a pre-trained model, we can generate phrases in the style of Sheldon Cooper (from *The Big Bang Theory*).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conceptual example of using a pre-trained model method\n",
        "class MockModel:\n",
        "    def generate_sheldon_phrase(self):\n",
        "        return \"'knock knock. penny. do you have an epost is part in your expert, too bealie to play the tariment with last night.'\"\n",
        "\n",
        "model = MockModel()\n",
        "print(model.generate_sheldon_phrase())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Modeling Decisions\n",
        "When building text generation models, you must decide on the granularity of your tokens:\n",
        "\n",
        "1.  **Word-level Tokens**:\n",
        "    *   **Pros**: Generates coherent words.\n",
        "    *   **Cons**: Requires massive datasets (hundreds of millions of sentences) to learn a robust vocabulary.\n",
        "2.  **Character-level Tokens**:\n",
        "    *   **Pros**: Can be trained faster; smaller vocabulary size.\n",
        "    *   **Cons**: Can generate typos or non-existent words.\n",
        "\n",
        "### Workflow\n",
        "1.  **Decide Token Type**: Characters vs. Words.\n",
        "2.  **Prepare Data**: Build training samples with `(past_tokens, next_token)` pairs.\n",
        "3.  **Design Architecture**: Embedding layers, LSTM layers, etc.\n",
        "4.  **Train & Experiment**: Iterate on hyperparameters.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. The Text Generating Function</span><br>\n",
        "\n",
        "To generate sentences, we feed the model a seed sequence, predict the next character, append it to the sequence, and repeat.\n",
        "\n",
        "### Sentence Structure\n",
        "*   Sentences are determined by punctuation (e.g., `.`, `!`, `?`).\n",
        "*   Punctuation marks must be included in the vocabulary.\n",
        "*   Special tokens like `<SENT>` and `</SENT>` can mark the beginning and end of sentences.\n",
        "\n",
        "### Generation Logic\n",
        "The following code demonstrates the logic loop for generating a sentence character by character.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Mocking necessary components for the example to run\n",
        "class MockKerasModel:\n",
        "    def predict(self, X):\n",
        "        # Returns a dummy probability distribution for 5 characters\n",
        "        return np.array([[0.1, 0.1, 0.1, 0.6, 0.1]])\n",
        "\n",
        "def index_to_char(index):\n",
        "    mapping = {0: 'a', 1: 'b', 2: 'c', 3: '.', 4: ' '}\n",
        "    return mapping.get(index, '')\n",
        "\n",
        "# Initialize variables\n",
        "model = MockKerasModel()\n",
        "sentence = ''\n",
        "next_char = ''\n",
        "X = np.zeros((1, 10, 5)) # Dummy input shape\n",
        "\n",
        "# Loop until end of sentence (period is detected)\n",
        "# Note: In a real scenario, you would update X with the new char in every iteration\n",
        "counter = 0 \n",
        "while next_char != '.' and counter < 10: # Added counter to prevent infinite loop in this mock\n",
        "    # Predict next char: Get pred array in position 0\n",
        "    pred = model.predict(X)[0]\n",
        "    \n",
        "    # Get index of highest probability\n",
        "    char_index = np.argmax(pred)\n",
        "    \n",
        "    # Convert index to character\n",
        "    next_char = index_to_char(char_index)\n",
        "    \n",
        "    # Concatenate to sentence\n",
        "    sentence = sentence + next_char\n",
        "    \n",
        "    counter += 1\n",
        "\n",
        "print(f\"Generated sentence segment: {sentence}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Probability Scaling (Temperature)</span><br>\n",
        "\n",
        "Instead of always picking the character with the absolute highest probability (which can be repetitive), we can sample from the probability distribution. **Temperature** is a hyperparameter that scales this distribution to control \"creativity.\"\n",
        "\n",
        "### Temperature Effects\n",
        "*   **Small values (< 1.0)**: Makes the distribution \"sharper.\" The model becomes more confident and conservative.\n",
        "*   **Value = 1.0**: No scaling.\n",
        "*   **Higher values (> 1.0)**: Flattens the distribution. The model becomes more random and \"creative,\" but potentially incoherent.\n",
        "\n",
        "### Implementation\n",
        "The function below implements the mathematical scaling using Logarithms and Exponentials.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def scale_softmax(softmax_pred, temperature=1.0):\n",
        "    # Take the logarithm\n",
        "    scaled_pred = np.log(softmax_pred) / temperature\n",
        "    \n",
        "    # Re-apply the exponential\n",
        "    scaled_pred = np.exp(scaled_pred)\n",
        "    \n",
        "    # Build probability distribution (normalize)\n",
        "    scaled_pred = scaled_pred / np.sum(scaled_pred)\n",
        "    \n",
        "    # Simulate multinomial sampling based on new distribution\n",
        "    # np.random.multinomial returns a one-hot vector, we take argmax to get the index\n",
        "    scaled_pred = np.random.multinomial(1, scaled_pred, 1)\n",
        "    \n",
        "    # Return simulated class index\n",
        "    return np.argmax(scaled_pred)\n",
        "\n",
        "# Example Usage\n",
        "dummy_preds = np.array([0.1, 0.2, 0.6, 0.1]) # High confidence in index 2\n",
        "print(f\"Original Max: {np.argmax(dummy_preds)}\")\n",
        "\n",
        "# Low temp (conservative)\n",
        "print(f\"Low Temp (0.1): {scale_softmax(dummy_preds, temperature=0.1)}\")\n",
        "\n",
        "# High temp (creative/random)\n",
        "print(f\"High Temp (2.0): {scale_softmax(dummy_preds, temperature=2.0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Text Generation Model Architecture</span><br>\n",
        "\n",
        "The architecture for text generation is similar to a classification model, but with specific nuances.\n",
        "\n",
        "### Key Characteristics\n",
        "*   **Classes**: The vocabulary size acts as the number of classes.\n",
        "*   **Output Layer**: Softmax activation with units equal to vocabulary size.\n",
        "*   **Loss Function**: `categorical_crossentropy`.\n",
        "*   **Evaluation**: We monitor loss, but \"accuracy\" is less relevant. Humans evaluate the quality of the generated text.\n",
        "\n",
        "### Keras Implementation\n",
        "Here is a standard LSTM-based architecture for text generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Hyperparameters (Example values)\n",
        "units = 128\n",
        "chars_window = 50  # Length of input sequence\n",
        "n_vocab = 40       # Size of vocabulary\n",
        "dropout = 0.15\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# First LSTM Layer: Must return sequences to feed the next LSTM layer\n",
        "model.add(LSTM(units, input_shape=(chars_window, n_vocab),\n",
        "               dropout=dropout, recurrent_dropout=dropout, return_sequences=True))\n",
        "\n",
        "# Second LSTM Layer: Returns only the last output (return_sequences=False by default)\n",
        "model.add(LSTM(units, dropout=dropout, recurrent_dropout=dropout,\n",
        "               return_sequences=False))\n",
        "\n",
        "# Output Layer: Predicts probability of next character\n",
        "model.add(Dense(n_vocab, activation='softmax'))\n",
        "\n",
        "# Compile\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> If the results are not good, try training for more epochs or adding complexity (more memory cells/units, more layers). </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. Neural Machine Translation (NMT) Overview</span><br>\n",
        "\n",
        "Neural Machine Translation (NMT) is a \"Many-to-Many\" problem where we map a sequence of words in one language to a sequence of words in another.\n",
        "\n",
        "### Example\n",
        "*   **Input**: \"Vamos jogar futebol?\" (Portuguese)\n",
        "*   **Output**: \"Let's go play soccer?\" (English)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conceptual usage\n",
        "# model.translate(\"Vamos jogar futebol?\")\n",
        "# Output: 'Let's go play soccer?'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### The Encoder-Decoder Architecture\n",
        "NMT models typically use two distinct parts:\n",
        "1.  **Encoder**: Processes the input sequence and compresses the information into a context vector (the internal state).\n",
        "2.  **Decoder**: Takes the context vector and generates the output sequence step-by-step.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 7. NMT: Encoder Architecture</span><br>\n",
        "\n",
        "The Encoder is responsible for reading the input language. It uses an Embedding layer to handle word vectors and an LSTM layer to capture sequential dependencies.\n",
        "\n",
        "### Key Component: RepeatVector\n",
        "The output of the Encoder is a single vector (the final state). However, the Decoder expects a sequence as input (one for each time step of the output sentence). `RepeatVector` bridges this gap by repeating the Encoder's output vector `N` times, where `N` is the length of the output sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, RepeatVector\n",
        "\n",
        "# Dimensions\n",
        "input_language_size = 1000\n",
        "input_wordvec_dim = 64\n",
        "input_language_len = 20\n",
        "output_language_len = 20\n",
        "\n",
        "# Instantiate the model\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer for input language\n",
        "# mask_zero=True ignores 0-padded values\n",
        "model.add(Embedding(input_language_size, input_wordvec_dim,\n",
        "                    input_length=input_language_len, mask_zero=True))\n",
        "\n",
        "# Add LSTM layer (The Encoder)\n",
        "model.add(LSTM(128))\n",
        "\n",
        "# Repeat the last vector to match the length of the output sequence\n",
        "model.add(RepeatVector(output_language_len))\n",
        "\n",
        "print(\"Encoder built successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 8. NMT: Decoder Architecture</span><br>\n",
        "\n",
        "The Decoder takes the repeated context vectors and generates the translated sentence.\n",
        "\n",
        "### Key Component: TimeDistributed\n",
        "We want to apply a Dense layer to *every* time step of the sequence independently to predict the word for that specific position. `TimeDistributed` wraps the Dense layer to apply it to every slice of the temporal input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TimeDistributed, Dense\n",
        "\n",
        "# Continuing from the previous model...\n",
        "eng_vocab_size = 2000\n",
        "\n",
        "# Add LSTM layer (The Decoder)\n",
        "# return_sequences=True is mandatory here because we need an output for every time step\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "\n",
        "# Add Time Distributed Dense layer\n",
        "# Applies the Dense layer to every time step to predict the word index\n",
        "model.add(TimeDistributed(Dense(eng_vocab_size, activation='softmax')))\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 9. Data Preparation for NMT</span><br>\n",
        "\n",
        "Data preparation for NMT involves tokenizing both the input language (e.g., Portuguese) and the output language (e.g., English), padding them to fixed lengths, and one-hot encoding the output for the classification task.\n",
        "\n",
        "### 9.1 Tokenize Input Language\n",
        "We convert text sentences into sequences of integers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Dummy Data\n",
        "input_texts_list = [\"Vamos jogar futebol\", \"Bom dia\"]\n",
        "length = 20\n",
        "\n",
        "# Use the Tokenizer class\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(input_texts_list)\n",
        "\n",
        "# Text to sequence of numerical indexes\n",
        "X = tokenizer.texts_to_sequences(input_texts_list)\n",
        "\n",
        "# Pad sequences (padding='post' adds zeros at the end)\n",
        "X = pad_sequences(X, maxlen=length, padding='post')\n",
        "\n",
        "print(f\"Input Shape: {X.shape}\")\n",
        "print(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 9.2 Tokenize Output Language\n",
        "Similar process for the target language.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dummy Data\n",
        "output_texts_list = [\"Let's play soccer\", \"Good morning\"]\n",
        "\n",
        "# Use the Tokenizer class\n",
        "tokenizer_out = Tokenizer()\n",
        "tokenizer_out.fit_on_texts(output_texts_list)\n",
        "\n",
        "# Text to sequence of numerical indexes\n",
        "Y = tokenizer_out.texts_to_sequences(output_texts_list)\n",
        "\n",
        "# Pad sequences\n",
        "Y = pad_sequences(Y, maxlen=length, padding='post')\n",
        "\n",
        "print(f\"Output Sequence Shape: {Y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 9.3 One-Hot Encode Output\n",
        "The output `Y` needs to be one-hot encoded because the final layer is a Dense layer with Softmax activation (classification).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Get vocab size from tokenizer\n",
        "vocab_size = len(tokenizer_out.word_index) + 1 \n",
        "\n",
        "# Instantiate a temporary variable\n",
        "ylist = list()\n",
        "\n",
        "# Loop over the sequence of numerical indexes\n",
        "for sequence in Y:\n",
        "    # One-hot encode each index on current sentence\n",
        "    encoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "    \n",
        "    # Append one-hot encoded values to the list\n",
        "    ylist.append(encoded)\n",
        "\n",
        "# Transform to np.array and reshape\n",
        "# Shape: (Number of samples, Sequence Length, Vocab Size)\n",
        "Y = np.array(ylist).reshape(Y.shape[0], Y.shape[1], vocab_size)\n",
        "\n",
        "print(f\"Final One-Hot Output Shape: {Y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 10. Training and Evaluation</span><br>\n",
        "\n",
        "### Training\n",
        "Training is performed using the standard Keras `fit` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# N is the number of epochs\n",
        "N = 5\n",
        "# model.fit(X, Y, epochs=N) \n",
        "print(\"Model training command: model.fit(X, Y, epochs=N)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Evaluation: BLEU Score\n",
        "Accuracy is not the best metric for translation. We use **BLEU (Bilingual Evaluation Understudy)**, which compares the machine-generated translation to one or more human reference translations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "# Example of how to access BLEU score in NLTK\n",
        "# nltk.translate.bleu_score.sentence_bleu(references, hypothesis)\n",
        "print(\"Evaluation metric: nltk.translate.bleu_score\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 11. Course Wrap-up and RNN Pitfalls</span><br>\n",
        "\n",
        "This section summarizes the entire course on Recurrent Neural Networks for Language Modeling.\n",
        "\n",
        "### Course Summary\n",
        "1.  **Introduction to Language Tasks**:\n",
        "    *   Sentiment Classification.\n",
        "    *   Multi-class Classification.\n",
        "    *   Text Generation.\n",
        "    *   Neural Machine Translation.\n",
        "2.  **Sequence to Sequence Models**: Understanding Many-to-Many architectures.\n",
        "3.  **Implementation**: Using Keras `Sequential`, `LSTM`, `Embedding`, and `TimeDistributed` layers.\n",
        "\n",
        "### RNN Pitfalls & Solutions\n",
        "*   **Vanishing/Exploding Gradients**: Standard RNNs struggle with long sequences because gradients can become zero or infinitely large during backpropagation.\n",
        "*   **Solution**: Use **GRU** (Gated Recurrent Unit) or **LSTM** (Long Short-Term Memory) cells, which have internal mechanisms (gates) to regulate information flow.\n",
        "\n",
        "### Other Applications\n",
        "*   **Name Creation**: Baby names, star names.\n",
        "*   **Marked Text Generation**: LaTeX, Markdown, XML, Code.\n",
        "*   **Chatbots**: Conversational agents.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 12. Conclusion</span><br>\n",
        "\n",
        "In this notebook, we explored the advanced capabilities of Recurrent Neural Networks, specifically focusing on **Sequence-to-Sequence** models.\n",
        "\n",
        "**Key Takeaways:**\n",
        "1.  **Text Generation**: We learned how to generate text character-by-character using LSTM networks and how to control the creativity of the output using **Temperature**.\n",
        "2.  **NMT Architecture**: We built a complete Encoder-Decoder architecture for Machine Translation using `RepeatVector` to bridge the gap between the encoder's summary and the decoder's sequence generation.\n",
        "3.  **Data Prep**: We mastered the complex preprocessing pipeline required for NMT, including dual tokenization and one-hot encoding of the target sequence.\n",
        "\n",
        "**Next Steps:**\n",
        "*   Experiment with different **Temperature** values to see how it affects text generation.\n",
        "*   Try replacing `LSTM` layers with `GRU` layers to compare training speed and performance.\n",
        "*   Apply the NMT architecture to a real-world dataset (e.g., the Anki project datasets) to build a functional translator.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}