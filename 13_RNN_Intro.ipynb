{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Recurrent Neural Networks (RNNs) for Language Modeling with Keras</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(From Theory to Implementation using TensorFlow/Keras)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction to the Course](#section-1)\n",
        "2. [Applications of Machine Learning to Text Data](#section-2)\n",
        "3. [Recurrent Neural Networks (RNNs) & Sequence Models](#section-3)\n",
        "4. [Introduction to Language Models](#section-4)\n",
        "5. [Preprocessing Text Data](#section-5)\n",
        "6. [Introduction to RNNs inside Keras](#section-6)\n",
        "7. [Building, Training, and Evaluating Models](#section-7)\n",
        "8. [Full Example: IMDB Sentiment Classification](#section-8)\n",
        "9. [Conclusion](#section-9)\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  üßæ 1. Introduction to the Course</span><br>\n",
        "\n",
        "Welcome to the comprehensive guide on **Recurrent Neural Networks (RNNs) for Language Modeling with Keras**. This notebook covers the fundamental concepts of processing text data, understanding sequence models, and implementing them using the Keras deep learning library.\n",
        "\n",
        "### The Abundance of Text Data\n",
        "Text data is ubiquitous in the modern digital landscape. Before diving into complex models, it is essential to recognize where this data comes from. Massive amounts of unstructured text are generated every second via:\n",
        "\n",
        "*   **News Outlets**: Yahoo! News, Google News.\n",
        "*   **Social Media**: Twitter (X), Facebook, Weibo.\n",
        "*   **Search Engines**: Google search queries.\n",
        "*   **General Web Content**: Articles, blogs, and forums.\n",
        "\n",
        "This data contains rich semantic structures, rules, and meanings that algorithms can learn to perform specific tasks.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  üßæ 2. Applications of Machine Learning to Text Data</span><br>\n",
        "\n",
        "Machine learning applied to text data generally falls into four major categories. Understanding these applications helps in selecting the right architecture.\n",
        "\n",
        "### 1. Sentiment Analysis\n",
        "Determining the emotional tone behind a body of text.\n",
        "*   **Input**: \"I loved this movie!\"\n",
        "*   **Output**: Positive (üëç) or Negative (üëé).\n",
        "\n",
        "### 2. Multi-class Classification\n",
        "Categorizing text into one of many specific topics.\n",
        "*   **Input**: A news article.\n",
        "*   **Output**: Politics, Sports, Science, Finance, etc.\n",
        "\n",
        "### 3. Text Generation\n",
        "Predicting the next word or sequence of words. This is used in features like \"Smart Compose\" in email clients.\n",
        "*   **Context**: \"Next World Cup is going to be awesome...\"\n",
        "*   **Suggested Replies**: \"Yes!\", \"No, I haven't.\", \"Not yet!\"\n",
        "\n",
        "### 4. Neural Machine Translation (NMT)\n",
        "Translating text from one language to another using neural networks.\n",
        "*   **Input (PT)**: \"Vamos jogar futebol esse domingo?\"\n",
        "*   **Output (EN)**: \"Let's play soccer this Sunday?\"\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  üßæ 3. Recurrent Neural Networks (RNNs) & Sequence Models</span><br>\n",
        "\n",
        "RNNs are designed to handle sequential data. Unlike traditional feedforward networks, RNNs have a \"memory\" that captures information about what has been calculated so far.\n",
        "\n",
        "### The RNN Cell\n",
        "In an RNN, the output of a cell at time step $t$ depends on the input at time $t$ and the hidden state from time $t-1$. This allows the network to share weights across time steps.\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> üí° <b>Tip:</b> In RNN architectures, weights are <b>shared</b> across all time steps. This significantly reduces the number of parameters compared to treating every word position as a separate feature. </div>\n",
        "\n",
        "### Sequence-to-Sequence Models\n",
        "\n",
        "We can configure RNNs in different ways depending on the input and output requirements:\n",
        "\n",
        "#### A. Many-to-One: Classification\n",
        "Used for Sentiment Analysis.\n",
        "*   **Input**: Sequence of words ($X_1, X_2, X_3, X_4$).\n",
        "*   **Processing**: The RNN processes the sequence.\n",
        "*   **Output**: A single prediction at the end ($Y_{pred}$).\n",
        "*   **Decision Rule**:\n",
        "    *   If $Y_{pred} > 0.5 \\rightarrow$ Positive.\n",
        "    *   Else $\\rightarrow$ Negative.\n",
        "\n",
        "#### B. Many-to-Many: Text Generation\n",
        "Used for Language Modeling.\n",
        "*   **Input**: Sequence of words.\n",
        "*   **Output**: A sequence where $Y_t$ is the prediction for the next word $X_{t+1}$.\n",
        "*   **Example**:\n",
        "    *   Input: \"The\" $\\rightarrow$ Output: \"weather\"\n",
        "    *   Input: \"weather\" $\\rightarrow$ Output: \"is\"\n",
        "\n",
        "#### C. Many-to-Many: Neural Machine Translation\n",
        "Uses an **Encoder-Decoder** architecture.\n",
        "*   **Encoder**: Processes the input sentence (e.g., \"Vamos jogar futebol\") and compresses it into a context vector.\n",
        "*   **Decoder**: Takes the context vector and generates the translated sentence (e.g., \"Let's play soccer\").\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  üßæ 4. Introduction to Language Models</span><br>\n",
        "\n",
        "A language model learns the probability of a sequence of words. It answers the question: *How likely is this sentence to occur?*\n",
        "\n",
        "### Sentence Probability Models\n",
        "\n",
        "Given the sentence: **\"I loved this movie\"**\n",
        "\n",
        "#### 1. Unigram Model\n",
        "Assumes words are independent.\n",
        "$$ P(\\text{sentence}) = P(\\text{I}) \\times P(\\text{loved}) \\times P(\\text{this}) \\times P(\\text{movie}) $$\n",
        "\n",
        "#### 2. N-gram Models\n",
        "Assumes the probability of a word depends on the previous $N-1$ words.\n",
        "\n",
        "*   **Bigram (N=2)**:\n",
        "    $$ P(\\text{sentence}) = P(\\text{I}) \\times P(\\text{loved} | \\text{I}) \\times P(\\text{this} | \\text{loved}) \\times P(\\text{movie} | \\text{this}) $$\n",
        "\n",
        "*   **Trigram (N=3)**:\n",
        "    $$ P(\\text{sentence}) = P(\\text{I}) \\times P(\\text{loved} | \\text{I}) \\times P(\\text{this} | \\text{I loved}) \\times \\dots $$\n",
        "\n",
        "#### 3. Skip-gram\n",
        "Looks at the context surrounding a word (both before and after).\n",
        "$$ P(\\text{sentence}) = P(\\text{context of I} | \\text{I}) \\times P(\\text{context of loved} | \\text{loved}) \\dots $$\n",
        "\n",
        "#### 4. Neural Networks (RNNs)\n",
        "In Deep Learning, the probability is often computed using a **Softmax** function on the output layer, which normalizes the outputs into a probability distribution over the vocabulary.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  üßæ 5. Preprocessing Text Data</span><br>\n",
        "\n",
        "Before feeding text into a neural network, it must be converted into numbers. This involves creating a vocabulary and mapping words to integers.\n",
        "\n",
        "### Step 1: Building Vocabulary Dictionaries\n",
        "We need two dictionaries:\n",
        "1.  `word_to_index`: Maps a word to a unique integer.\n",
        "2.  `index_to_word`: Maps the integer back to the word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Original Code Implementation from Slides\n",
        "text = \"i loved this movie i loved this actor\" # Dummy text for demonstration\n",
        "\n",
        "# Get unique words\n",
        "unique_words = list(set(text.split(' ')))\n",
        "\n",
        "# Create dictionary: word is key, index is value\n",
        "word_to_index = {k:v for (v,k) in enumerate(unique_words)}\n",
        "\n",
        "# Create dictionary: index is key, word is value\n",
        "index_to_word = {k:v for (k,v) in enumerate(unique_words)}\n",
        "\n",
        "print(\"Word to Index:\", word_to_index)\n",
        "print(\"Index to Word:\", index_to_word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Step 2: Creating Input (X) and Output (y)\n",
        "For a language model, we often use a sliding window. If `sentence_size` is 3, we use 3 words to predict the 4th.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize variables\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "sentence_size = 3\n",
        "step = 1\n",
        "\n",
        "# Loop over the text\n",
        "# We stop at len(text) - sentence_size to ensure we have a target label\n",
        "words = text.split(' ') # Working with list of words, not characters for this example\n",
        "\n",
        "for i in range(0, len(words) - sentence_size, step):\n",
        "    # Append the sequence of words (inputs)\n",
        "    X.append(words[i : i + sentence_size])\n",
        "    # Append the next word (target)\n",
        "    y.append(words[i + sentence_size])\n",
        "\n",
        "# Example output\n",
        "print(f\"Input X[0]: {X[0]}\")\n",
        "print(f\"Target y[0]: {y[0]}\")\n",
        "\n",
        "# Converting to Indices (Preprocessing for Model)\n",
        "X_indices = []\n",
        "y_indices = []\n",
        "\n",
        "for i in range(len(X)):\n",
        "    sentence_indices = [word_to_index[w] for w in X[i]]\n",
        "    target_index = word_to_index[y[i]]\n",
        "    X_indices.append(sentence_indices)\n",
        "    y_indices.append(target_index)\n",
        "\n",
        "print(f\"Numerical X[0]: {X_indices[0]}\")\n",
        "print(f\"Numerical y[0]: {y_indices[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Step 3: Transforming New Texts\n",
        "When the model is in production, new raw text must undergo the exact same transformation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_text = [\"i loved this\"] # Example new sentence\n",
        "\n",
        "new_text_split = []\n",
        "\n",
        "for sentence in new_text:\n",
        "    sent_split = []\n",
        "    for wd in sentence.split(' '):\n",
        "        # Check if word exists in vocab to avoid errors\n",
        "        if wd in word_to_index:\n",
        "            ix = word_to_index[wd]\n",
        "            sent_split.append(ix)\n",
        "    new_text_split.append(sent_split)\n",
        "\n",
        "print(\"New Text Indices:\", new_text_split)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  üßæ 6. Introduction to RNNs inside Keras</span><br>\n",
        "\n",
        "**Keras** is a high-level neural networks API, written in Python and capable of running on top of TensorFlow. It is designed to enable fast experimentation.\n",
        "\n",
        "### Key Keras Modules\n",
        "\n",
        "| Module | Description |\n",
        "| :--- | :--- |\n",
        "| `keras.models` | Contains `Sequential` (linear stack of layers) and `Model` (functional API). |\n",
        "| `keras.layers` | Contains building blocks like `LSTM`, `GRU`, `Dense`, `Embedding`, `Dropout`. |\n",
        "| `keras.preprocessing` | Utilities for sequence padding (`pad_sequences`) and text tokenization. |\n",
        "| `keras.datasets` | Pre-loaded datasets like IMDB Movie Reviews and Reuters Newswire. |\n",
        "\n",
        "### Important Layers for NLP\n",
        "\n",
        "1.  **Embedding**: Turns positive integers (indexes) into dense vectors of fixed size.\n",
        "2.  **LSTM / GRU**: Recurrent layers that handle sequence data.\n",
        "3.  **Dense**: Regular fully connected layer.\n",
        "4.  **Bidirectional**: Wrapper that allows RNNs to process input from start-to-end and end-to-start.\n",
        "\n",
        "### Padding Sequences\n",
        "RNNs usually expect inputs of the same length. `pad_sequences` handles this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Example: Different length sentences\n",
        "sequences = [\n",
        "    [1, 2, 3],       # Length 3\n",
        "    [1, 2, 3, 4, 5], # Length 5\n",
        "    [1]              # Length 1\n",
        "]\n",
        "\n",
        "# Pad to maxlen=3 (truncating longer ones, padding shorter ones)\n",
        "padded = pad_sequences(sequences, maxlen=3)\n",
        "print(\"Padded Sequences:\\n\", padded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  üßæ 7. Building, Training, and Evaluating Models</span><br>\n",
        "\n",
        "Here is the standard workflow for creating a neural network in Keras.\n",
        "\n",
        "### 1. Creating the Model\n",
        "We use the `Sequential` API to stack layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Instantiate\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers\n",
        "# input_dim=100 implies we have 100 features\n",
        "model.add(Dense(64, activation='relu', input_dim=100)) \n",
        "model.add(Dense(1, activation='sigmoid')) # Binary classification output\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mean_squared_error', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 2. Training the Model\n",
        "We use the `.fit()` method.\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> üí° <b>Tip:</b> <br><b>Epochs</b>: One pass over the entire dataset.<br><b>Batch Size</b>: Number of samples per gradient update. </div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mock data for demonstration\n",
        "import numpy as np\n",
        "X_train = np.random.random((1000, 100))\n",
        "y_train = np.random.randint(2, size=(1000, 1))\n",
        "\n",
        "# Train\n",
        "model.fit(X_train, y_train, epochs=2, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 3. Evaluation and Prediction\n",
        "Once trained, we check performance on test data and make predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mock test data\n",
        "X_test = np.random.random((200, 100))\n",
        "y_test = np.random.randint(2, size=(200, 1))\n",
        "\n",
        "# Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Predict\n",
        "new_data = np.random.random((2, 100))\n",
        "predictions = model.predict(new_data)\n",
        "print(\"Predictions:\", predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  üßæ 8. Full Example: IMDB Sentiment Classification</span><br>\n",
        "\n",
        "This section combines everything into a full RNN example using the IMDB dataset. The goal is to classify movie reviews as positive or negative.\n",
        "\n",
        "### The Architecture\n",
        "1.  **Embedding Layer**: Converts word indices to vectors (Vocab size 10,000 -> Vector size 128).\n",
        "2.  **LSTM Layer**: 128 units, with dropout to prevent overfitting.\n",
        "3.  **Dense Layer**: 1 unit with Sigmoid activation (output 0 to 1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# 1. Load Data (Limiting to top 10,000 words)\n",
        "max_features = 10000\n",
        "maxlen = 80  # Cut texts after this number of words (among top max_features most common words)\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "# 2. Pad Sequences\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "# 3. Build and Compile the Model\n",
        "print('Building model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) # recurrent_dropout is optional but good\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 4. Training\n",
        "print('Training...')\n",
        "# Using a small epoch count and batch size for demonstration speed\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=1, validation_data=(x_test, y_test))\n",
        "\n",
        "# 5. Evaluation\n",
        "score, acc = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  üßæ 9. Conclusion</span><br>\n",
        "\n",
        "In this notebook, we have traversed the landscape of Recurrent Neural Networks for language modeling.\n",
        "\n",
        "**Key Takeaways:**\n",
        "1.  **Text Data is Sequential**: Unlike images, text has a temporal dimension where order matters.\n",
        "2.  **RNNs are Specialized**: They maintain a hidden state (memory) to process sequences, making them ideal for NLP.\n",
        "3.  **Preprocessing is Vital**: Raw text must be tokenized, indexed, and padded before entering a network.\n",
        "4.  **Keras Simplifies Deep Learning**: With just a few lines of code, we can build complex architectures like Embeddings + LSTMs to perform sentiment analysis with high accuracy.\n",
        "\n",
        "**Next Steps:**\n",
        "*   Experiment with **Bidirectional LSTMs** to capture context from both directions.\n",
        "*   Try **Text Generation** by changing the output to a Softmax over the vocabulary size.\n",
        "*   Explore **Transformers** (like BERT or GPT), which have largely superseded RNNs for complex tasks, though RNNs remain a foundational concept.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}