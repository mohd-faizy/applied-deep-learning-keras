{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>GOING DEEPER: IMAGE MODELING WITH KERAS</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Deep Learning Architectures, Parameter Counting, and Pooling)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "1. [Building Deep Networks](#section-1)\n",
        "2. [Why Do We Want Deep Networks?](#section-2)\n",
        "3. [How Many Parameters?](#section-3)\n",
        "4. [Reducing Parameters with Pooling](#section-4)\n",
        "5. [Conclusion](#section-5)\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. BUILDING DEEP NETWORKS</span><br>\n",
        "\n",
        "### Introduction to Deep Architectures\n",
        "In image modeling, we often start with a simple Convolutional Neural Network (CNN). A basic network might consist of a single convolutional layer followed by a flattening step and a dense output layer. However, to capture more complex patterns, we need to \"go deeper\" by stacking multiple convolutional layers.\n",
        "\n",
        "### Single Convolutional Layer\n",
        "The fundamental building block involves a `Conv2D` layer, which extracts features using filters (kernels), followed by `Flatten` to convert the 2D feature maps into a 1D vector, and finally a `Dense` layer for classification.\n",
        "\n",
        "#### Original Code (From PDF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(10, kernel_size=2, activation='relu',\n",
        "                 input_shape=(img_rows, img_cols, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(3, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Building a Deeper Network\n",
        "To build a deeper network, we add more `Conv2D` layers before flattening. This allows the network to process the output of the first convolution, effectively creating a hierarchy of features.\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> In the PDF, the code snippet uses <code>padding='equal'</code>. This is likely a typo or pseudocode. In Keras, the standard padding options are <code>'valid'</code> (no padding) or <code>'same'</code> (output size equals input size). We will use <code>'same'</code> in the executable code below. </div>\n",
        "\n",
        "#### Enhanced Executable Code\n",
        "Below is a complete, runnable example of building both a shallow and a deep CNN using Keras.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
        "\n",
        "# Define dummy image dimensions for the example\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# --- 1. Shallow Network ---\n",
        "print(\"Building Shallow Network...\")\n",
        "model_shallow = Sequential()\n",
        "model_shallow.add(Conv2D(10, kernel_size=2, activation='relu', \n",
        "                         input_shape=(img_rows, img_cols, 1)))\n",
        "model_shallow.add(Flatten())\n",
        "model_shallow.add(Dense(3, activation='softmax'))\n",
        "model_shallow.summary()\n",
        "\n",
        "# --- 2. Deep Network ---\n",
        "print(\"\\nBuilding Deep Network...\")\n",
        "model_deep = Sequential()\n",
        "\n",
        "# First convolutional layer\n",
        "model_deep.add(Conv2D(10, kernel_size=2, activation='relu', \n",
        "                      input_shape=(img_rows, img_cols, 1), \n",
        "                      padding='same')) # Corrected from 'equal' to 'same'\n",
        "\n",
        "# Second convolutional layer\n",
        "model_deep.add(Conv2D(10, kernel_size=2, activation='relu'))\n",
        "\n",
        "model_deep.add(Flatten())\n",
        "model_deep.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model_deep.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. WHY DO WE WANT DEEP NETWORKS?</span><br>\n",
        "\n",
        "### Feature Hierarchy\n",
        "Deep networks are powerful because they learn representations of data in a hierarchical fashion. As data flows through the layers, the network extracts increasingly complex features.\n",
        "\n",
        "| Layer Depth | Feature Complexity | Visual Examples |\n",
        "| :--- | :--- | :--- |\n",
        "| **Early Layers** | Low-level features | Edges, lines, simple textures, diagonal gradients. |\n",
        "| **Intermediate Layers** | Mid-level features | Patterns, curves, simple shapes (circles, squares), eyes, corners. |\n",
        "| **Late Layers** | High-level features | Complex objects, faces, buildings, entire distinct entities. |\n",
        "\n",
        "### Trade-offs\n",
        "While depth improves the model's ability to understand complex data, it comes with costs:\n",
        "1.  **Computational Cost:** More layers mean more operations (multiplications and additions), requiring more powerful hardware (GPUs) and longer training times.\n",
        "2.  **Data Requirements:** Deeper networks generally have more parameters, which increases the risk of overfitting unless significantly more training data is provided.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. HOW MANY PARAMETERS?</span><br>\n",
        "\n",
        "Understanding the number of parameters is crucial for managing model size and computational efficiency. The calculation differs between Dense (Fully Connected) layers and Convolutional layers.\n",
        "\n",
        "### 3.1 Counting Parameters in Dense Layers\n",
        "\n",
        "For a Dense layer, every input unit is connected to every output unit.\n",
        "\n",
        "**Formula:**\n",
        "$$ Parameters = (N_{in} \\times N_{out}) + N_{bias} $$\n",
        "*   $N_{in}$: Number of input units.\n",
        "*   $N_{out}$: Number of units in the current layer.\n",
        "*   $N_{bias}$: One bias term per output unit ($N_{out}$).\n",
        "\n",
        "#### Example Calculation (From PDF)\n",
        "Consider a model with:\n",
        "1.  Input: 784 units\n",
        "2.  Dense Layer 1: 10 units\n",
        "3.  Dense Layer 2: 10 units\n",
        "4.  Output Layer: 3 units\n",
        "\n",
        "*   **Layer 1:** $784 \\times 10 + 10 = 7850$\n",
        "*   **Layer 2:** $10 \\times 10 + 10 = 110$\n",
        "*   **Output:** $10 \\times 3 + 3 = 33$\n",
        "*   **Total:** $7850 + 110 + 33 = 7993$\n",
        "\n",
        "#### Executable Code: Dense Parameter Count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dense = Sequential()\n",
        "model_dense.add(Dense(10, activation='relu', input_shape=(784,)))\n",
        "model_dense.add(Dense(10, activation='relu'))\n",
        "model_dense.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Verify the calculation\n",
        "model_dense.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 3.2 Counting Parameters in CNNs\n",
        "\n",
        "Convolutional layers share weights across the image, making them much more parameter-efficient than Dense layers.\n",
        "\n",
        "**Formula:**\n",
        "$$ Parameters = (K_h \\times K_w \\times C_{in} \\times C_{out}) + C_{bias} $$\n",
        "*   $K_h, K_w$: Kernel height and width.\n",
        "*   $C_{in}$: Number of input channels (depth).\n",
        "*   $C_{out}$: Number of output filters (depth).\n",
        "*   $C_{bias}$: One bias term per output filter ($C_{out}$).\n",
        "\n",
        "#### Example Calculation (From PDF)\n",
        "Consider a CNN with:\n",
        "1.  Input: $28 \\times 28 \\times 1$\n",
        "2.  Conv2D Layer 1: 10 filters, kernel size 3.\n",
        "3.  Conv2D Layer 2: 10 filters, kernel size 3.\n",
        "4.  Flatten\n",
        "5.  Dense Output: 3 units.\n",
        "\n",
        "*   **Conv2D_1:** $3 \\times 3 \\times 1 \\text{ (input)} \\times 10 \\text{ (filters)} + 10 = 100$\n",
        "*   **Conv2D_2:** $3 \\times 3 \\times 10 \\text{ (input from prev)} \\times 10 \\text{ (filters)} + 10 = 910$\n",
        "*   **Flatten:** 0 parameters (just reshaping).\n",
        "*   **Dense:** Input size is $28 \\times 28 \\times 10 = 7840$ (assuming padding='same').\n",
        "    $7840 \\times 3 + 3 = 23,523$\n",
        "*   **Total:** $100 + 910 + 0 + 23,523 = 24,533$\n",
        "\n",
        "#### Executable Code: CNN Parameter Count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_cnn = Sequential()\n",
        "# Layer 1\n",
        "model_cnn.add(Conv2D(10, kernel_size=3, activation='relu', \n",
        "                     input_shape=(28, 28, 1), padding='same'))\n",
        "# Layer 2\n",
        "model_cnn.add(Conv2D(10, kernel_size=3, activation='relu', \n",
        "                     padding='same'))\n",
        "# Flatten\n",
        "model_cnn.add(Flatten())\n",
        "# Output\n",
        "model_cnn.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Verify the calculation\n",
        "model_cnn.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 3.3 Increasing Units/Filters\n",
        "Increasing the number of units or filters drastically increases the parameter count, especially in the Dense layers following a Flatten operation.\n",
        "\n",
        "**Example Comparison:**\n",
        "If we increase the filters in the CNN example above to 5 (layer 1) and 15 (layer 2):\n",
        "*   Flatten output becomes: $28 \\times 28 \\times 15 = 11,760$.\n",
        "*   Dense layer params: $11,760 \\times 3 + 3 = 35,283$.\n",
        "*   Total params jump significantly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# High parameter model example\n",
        "model_large = Sequential()\n",
        "model_large.add(Conv2D(5, kernel_size=3, activation='relu', \n",
        "                       input_shape=(28, 28, 1), padding='same'))\n",
        "model_large.add(Conv2D(15, kernel_size=3, activation='relu', \n",
        "                       padding='same'))\n",
        "model_large.add(Flatten())\n",
        "model_large.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model_large.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. REDUCING PARAMETERS WITH POOLING</span><br>\n",
        "\n",
        "### The Problem\n",
        "As seen in the previous section, the `Flatten` layer can result in a massive vector if the spatial dimensions ($Height \\times Width$) of the feature maps are large. This leads to an explosion of parameters in the subsequent Dense layer.\n",
        "\n",
        "### The Solution: Max Pooling\n",
        "Pooling layers reduce the spatial dimensions of the input volume. **Max Pooling** is the most common type. It operates by sliding a window over the input and taking the maximum value within that window.\n",
        "\n",
        "*   **Effect:** Reduces image size (e.g., halves height and width).\n",
        "*   **Parameters:** 0 (Pooling has no learnable weights).\n",
        "*   **Benefit:** Summarizes features and provides translation invariance.\n",
        "\n",
        "### Manual Implementation (NumPy)\n",
        "To understand how Max Pooling works, we can implement it manually using NumPy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a dummy image (4x6)\n",
        "im = np.array([\n",
        "    [1, 3, 2, 4, 1, 0],\n",
        "    [5, 2, 1, 0, 3, 2],\n",
        "    [4, 1, 0, 5, 2, 1],\n",
        "    [2, 6, 3, 1, 4, 0]\n",
        "])\n",
        "\n",
        "print(\"Original Image Shape:\", im.shape)\n",
        "print(im)\n",
        "\n",
        "# Initialize result array (half the size)\n",
        "result = np.zeros((im.shape[0]//2, im.shape[1]//2))\n",
        "\n",
        "# Manual Max Pooling (Window size 2x2, Stride 2)\n",
        "for ii in range(result.shape[0]):\n",
        "    for jj in range(result.shape[1]):\n",
        "        # Extract 2x2 window\n",
        "        window = im[ii*2 : ii*2+2, jj*2 : jj*2+2]\n",
        "        # Take max\n",
        "        result[ii, jj] = np.max(window)\n",
        "\n",
        "print(\"\\nPooled Image Shape:\", result.shape)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Keras Implementation\n",
        "In Keras, we use the `MaxPool2D` layer.\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> Adding pooling layers significantly reduces the number of parameters in the final Dense layer because the input to <code>Flatten</code> is much smaller. </div>\n",
        "\n",
        "#### Executable Code: CNN with Max Pooling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import MaxPool2D\n",
        "\n",
        "model_pool = Sequential()\n",
        "\n",
        "# Layer 1: Conv + Pool\n",
        "model_pool.add(Conv2D(5, kernel_size=3, activation='relu', \n",
        "                      input_shape=(28, 28, 1)))\n",
        "model_pool.add(MaxPool2D(2)) # Reduces 28x28 -> 14x14 (approx, depending on padding)\n",
        "\n",
        "# Layer 2: Conv + Pool\n",
        "model_pool.add(Conv2D(15, kernel_size=3, activation='relu'))\n",
        "model_pool.add(MaxPool2D(2)) # Reduces further\n",
        "\n",
        "model_pool.add(Flatten())\n",
        "model_pool.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Observe the reduction in parameters compared to the previous section\n",
        "model_pool.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Analysis of Output:**\n",
        "1.  Notice the `Output Shape` decreases after every `MaxPooling2D` layer.\n",
        "2.  Notice the `Param #` for pooling layers is 0.\n",
        "3.  Notice the final `Dense` layer has significantly fewer parameters compared to the model without pooling.\n",
        "\n",
        "***\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. CONCLUSION</span><br>\n",
        "\n",
        "In this notebook, we explored the mechanics of \"going deeper\" with Keras image models.\n",
        "\n",
        "**Key Takeaways:**\n",
        "1.  **Depth Matters:** Deeper networks allow the model to learn a hierarchy of features, from simple edges to complex objects.\n",
        "2.  **Parameter Counting:**\n",
        "    *   **Dense Layers:** Parameters grow multiplicatively with input and output size ($N_{in} \\times N_{out}$).\n",
        "    *   **Conv Layers:** Parameters depend on kernel size and depth, not image size ($K \\times K \\times C_{in} \\times C_{out}$), making them efficient.\n",
        "3.  **Pooling is Essential:** Max Pooling reduces the spatial dimensions of feature maps. This controls the explosion of parameters in the final dense layers, reduces computational cost, and helps prevent overfitting.\n",
        "\n",
        "**Next Steps:**\n",
        "*   Experiment with different kernel sizes (e.g., 5x5 vs 3x3).\n",
        "*   Try adding `Dropout` layers to further prevent overfitting in deep networks.\n",
        "*   Apply these architectures to real-world datasets like MNIST or CIFAR-10.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}