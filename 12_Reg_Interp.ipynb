{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>Image Modeling with Keras: Tracking, Regularization, and Interpretation</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Tracking Learning, Regularization, and Model Interpretation)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Tracking Learning](#section-1)\n",
        "2. [Neural Network Regularization](#section-2)\n",
        "3. [Interpreting the Model](#section-3)\n",
        "4. [Future Directions & Advanced Architectures](#section-4)\n",
        "5. [Conclusion](#conclusion)\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"section-1\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. Tracking Learning</span><br>\n",
        "\n",
        "Training a deep learning model is an iterative process. To ensure the model is learning patterns rather than memorizing data, we must monitor its performance over time. This involves analyzing learning curves and storing the best version of the model.\n",
        "\n",
        "### 1.1 Learning Curves\n",
        "Learning curves visualize the model's performance (usually Loss or Accuracy) over epochs.\n",
        "\n",
        "*   **Training Curve:** Shows how well the model fits the training data. Ideally, this decreases steadily.\n",
        "*   **Validation Curve:** Shows how well the model generalizes to unseen data.\n",
        "*   **Overfitting:** Occurs when the training loss continues to decrease, but the validation loss begins to increase. This indicates the model is memorizing the training set noise.\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> The `model.fit()` method in Keras returns a `History` object. This object contains a dictionary `history.history` with the loss and metrics per epoch, which is essential for plotting. </div>\n",
        "\n",
        "#### Original Code (from PDF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training = model.fit(train_data, train_labels,\n",
        "                     epochs=3, validation_split=0.2)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(training.history['loss'])\n",
        "plt.plot(training.history['val_loss'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Enhanced Code (Runnable Example)\n",
        "We will simulate this using the Fashion-MNIST dataset to generate real learning curves.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# 1. Prepare Data\n",
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
        "train_data = train_data.reshape((60000, 28, 28, 1)) / 255.0\n",
        "test_data = test_data.reshape((10000, 28, 28, 1)) / 255.0\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "# 2. Build a Simple Model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(10, kernel_size=3, activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 3. Train and Capture History\n",
        "# We use a small number of epochs for demonstration\n",
        "training = model.fit(train_data, train_labels,\n",
        "                     epochs=5, \n",
        "                     validation_split=0.2,\n",
        "                     verbose=1)\n",
        "\n",
        "# 4. Plotting\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(training.history['loss'], label='Training Loss')\n",
        "plt.plot(training.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Learning Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 1.2 Storing Optimal Parameters\n",
        "Because models can overfit if trained too long, we want to save the model weights at the point where performance on the validation set was best, not necessarily the final epoch. Keras provides `ModelCheckpoint` for this.\n",
        "\n",
        "#### Original Code (from PDF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# This checkpoint object will store the model parameters\n",
        "# in the file \"weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint('weights.hdf5', monitor='val_loss',\n",
        "                             save_best_only=True)\n",
        "\n",
        "# Store in a list to be used during training\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit the model on a training set, using the checkpoint as a\n",
        "# callback\n",
        "model.fit(train_data, train_labels, validation_split=0.2,\n",
        "          epochs=3, callbacks=callbacks_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 1.3 Loading Stored Parameters\n",
        "Once training is complete (or interrupted), you can load the best weights saved by the checkpoint to ensure you are using the optimal version of the model.\n",
        "\n",
        "#### Original Code (from PDF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_weights('weights.hdf5')\n",
        "model.predict_classes(test_data)\n",
        "# Output example: array([2, 2, 1, 2, 0, 1, 0, 1, 2, 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Enhanced Code (Runnable)\n",
        "*Note: `predict_classes` is deprecated in newer Keras versions. We use `np.argmax(model.predict(...), axis=-1)` instead.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# 1. Define Checkpoint\n",
        "checkpoint = ModelCheckpoint('best_weights.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# 2. Train with Callback\n",
        "model.fit(train_data, train_labels, \n",
        "          validation_split=0.2,\n",
        "          epochs=3, \n",
        "          callbacks=[checkpoint],\n",
        "          verbose=0)\n",
        "\n",
        "# 3. Load the best weights\n",
        "model.load_weights('best_weights.keras')\n",
        "\n",
        "# 4. Predict\n",
        "predictions = model.predict(test_data[:5])\n",
        "predicted_classes = np.argmax(predictions, axis=-1)\n",
        "\n",
        "print(f\"Predicted classes for first 5 images: {predicted_classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<a id=\"section-2\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. Neural Network Regularization</span><br>\n",
        "\n",
        "Regularization techniques are used to prevent overfitting and improve the generalization of neural networks.\n",
        "\n",
        "### 2.1 Dropout\n",
        "Dropout is a technique where, during each learning step, a random subset of units (neurons) is ignored.\n",
        "*   **Mechanism:** Select a subset of units -> Ignore in forward pass -> Ignore in back-propagation.\n",
        "*   **Effect:** Prevents neurons from co-adapting too much; forces the network to learn robust features.\n",
        "\n",
        "#### Original Code (Dropout in Keras)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(5, kernel_size=3, activation='relu',\n",
        "                 input_shape=(img_rows, img_cols, 1)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(15, kernel_size=3, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(3, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 2.2 Batch Normalization\n",
        "Batch normalization rescales the outputs of a layer to have a mean of 0 and a standard deviation of 1. This stabilizes the learning process and often accelerates convergence.\n",
        "\n",
        "#### Original Code (Batch Normalization in Keras)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, BatchNormalization\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(5, kernel_size=3, activation='relu',\n",
        "                 input_shape=(img_rows, img_cols, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(15, kernel_size=3, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(3, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> <b>Disharmony Warning:</b> Be careful when using Dropout and Batch Normalization together. Because Batch Normalization relies on the statistics of the batch, and Dropout randomly changes the active units, using them in immediate succession can sometimes lead to worse performance (disharmony).</div>\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"section-3\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. Interpreting the Model</span><br>\n",
        "\n",
        "Deep learning models are often called \"black boxes,\" but we can inspect their internal components (layers and weights) to understand what they are learning.\n",
        "\n",
        "### 3.1 Selecting Layers and Getting Weights\n",
        "We can access specific layers via `model.layers` and extract their learned parameters (kernels/filters) using `get_weights()`.\n",
        "\n",
        "#### Original Code (Inspection)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selecting layers\n",
        "model.layers\n",
        "# Output: [<keras.layers.convolutional.Conv2D...>, ...]\n",
        "\n",
        "# Getting weights\n",
        "conv1 = model.layers[0]\n",
        "weights1 = conv1.get_weights()\n",
        "len(weights1) # 2 (Weights and Biases)\n",
        "\n",
        "kernels1 = weights1[0]\n",
        "kernels1.shape # (3, 3, 1, 5) -> (Height, Width, Channels, Filters)\n",
        "\n",
        "# Extracting a specific kernel (1st filter)\n",
        "kernel1_1 = kernels1[:, :, 0, 0]\n",
        "kernel1_1.shape # (3, 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 3.2 Visualizing the Kernel\n",
        "We can visualize the actual filter matrix as an image. This shows us the pattern the filter is looking for (e.g., edges, diagonals).\n",
        "\n",
        "#### Original Code (Visualization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.imshow(kernel1_1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 3.3 Visualizing Kernel Responses (Convolutions)\n",
        "To understand what a filter does, we can apply it (convolve it) with a test image. This creates a \"feature map\" or \"filtered image\" highlighting where the pattern exists in the image.\n",
        "\n",
        "#### Original Code (Convolution Response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a test image (e.g., a sneaker)\n",
        "test_image = test_data[3, :, :, 0]\n",
        "plt.imshow(test_image)\n",
        "\n",
        "# Apply convolution (Function 'convolution' assumed to exist in context)\n",
        "filtered_image = convolution(test_image, kernel1_1)\n",
        "plt.imshow(filtered_image)\n",
        "\n",
        "# Another example (e.g., a shirt)\n",
        "test_image_2 = test_data[4, :, :, 1] # Note: Indexing might vary based on data shape\n",
        "plt.imshow(test_image_2)\n",
        "filtered_image_2 = convolution(test_image_2, kernel1_1)\n",
        "plt.imshow(filtered_image_2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Enhanced Code (Runnable with Convolution Helper)\n",
        "Since the PDF does not define the `convolution` function, we must implement it using `scipy` or `numpy` to make this section executable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.signal import convolve2d\n",
        "\n",
        "# 1. Setup a model with known shape to match PDF example\n",
        "# Input: 28x28x1, Conv2D with 5 filters of size 3x3\n",
        "model_interp = Sequential()\n",
        "model_interp.add(Conv2D(5, kernel_size=3, activation='relu', input_shape=(28, 28, 1)))\n",
        "model_interp.build() \n",
        "\n",
        "# 2. Extract Kernels\n",
        "conv1 = model_interp.layers[0]\n",
        "weights1 = conv1.get_weights()\n",
        "kernels = weights1[0] # Shape (3, 3, 1, 5)\n",
        "\n",
        "# Select the first kernel\n",
        "kernel_1 = kernels[:, :, 0, 0]\n",
        "\n",
        "# 3. Select a Test Image (from Fashion MNIST loaded earlier)\n",
        "# Index 0 is usually a boot/shoe in Fashion MNIST\n",
        "test_img = test_data[0, :, :, 0] \n",
        "\n",
        "# 4. Define Convolution Function\n",
        "def convolution(image, kernel):\n",
        "    # Use 'valid' mode to match typical Keras Conv2D behavior without padding\n",
        "    return convolve2d(image, kernel, mode='valid')\n",
        "\n",
        "# 5. Apply Convolution\n",
        "filtered_img = convolution(test_img, kernel_1)\n",
        "\n",
        "# 6. Visualize\n",
        "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Original Kernel\n",
        "ax[0].imshow(kernel_1, cmap='viridis')\n",
        "ax[0].set_title(\"Learned Kernel (3x3)\")\n",
        "\n",
        "# Original Image\n",
        "ax[1].imshow(test_img, cmap='viridis')\n",
        "ax[1].set_title(\"Original Image\")\n",
        "\n",
        "# Filtered Image (Response)\n",
        "ax[2].imshow(filtered_img, cmap='viridis')\n",
        "ax[2].set_title(\"Filtered Image (Response)\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<a id=\"section-4\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. Future Directions & Advanced Architectures</span><br>\n",
        "\n",
        "After mastering the basics of CNNs, regularization, and interpretation, the field of Deep Learning expands into more complex architectures.\n",
        "\n",
        "### 4.1 Residual Networks (ResNets)\n",
        "As networks get deeper, they become harder to train due to vanishing gradients.\n",
        "*   **Concept:** Residual networks introduce \"skip connections\" (or identity shortcuts).\n",
        "*   **Mechanism:** The input $x$ is added to the output of a weight layer: $\\mathcal{F}(x) + x$.\n",
        "*   **Benefit:** Allows training of extremely deep networks (hundreds of layers).\n",
        "\n",
        "### 4.2 Transfer Learning\n",
        "Instead of training a network from scratch, Transfer Learning involves taking a pre-trained network (trained on a massive dataset like ImageNet) and fine-tuning it for a specific task. This saves time and requires less data.\n",
        "\n",
        "### 4.3 Fully Convolutional Networks (FCNs)\n",
        "FCNs are used for tasks like semantic segmentation.\n",
        "*   **Structure:** They replace dense (fully connected) layers with convolutional layers.\n",
        "*   **Operation:** They often use upsampling (deconvolution) to generate an output map that is the same size as the input image, classifying every pixel rather than the whole image.\n",
        "\n",
        "### 4.4 Generative Adversarial Networks (GANs)\n",
        "GANs involve two networks competing against each other:\n",
        "1.  **Generator:** Creates fake images.\n",
        "2.  **Discriminator:** Tries to distinguish between real and fake images.\n",
        "*   **Result:** The generator learns to create highly realistic synthetic images.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"conclusion\"></a>\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. Conclusion</span><br>\n",
        "\n",
        "In this notebook, we have covered the essential lifecycle of building, improving, and understanding Convolutional Neural Networks (CNNs) using Keras.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "| Concept | Description |\n",
        "| :--- | :--- |\n",
        "| **Image Classification** | The fundamental task of assigning labels to images. |\n",
        "| **Convolutions** | The core operation for feature extraction in images. |\n",
        "| **Parameter Reduction** | Using Pooling layers and tweaking convolution sizes to make models efficient. |\n",
        "| **Regularization** | Using **Dropout** and **Batch Normalization** to prevent overfitting and improve stability. |\n",
        "| **Monitoring** | Using **Learning Curves** (Loss/Accuracy) to track training vs. validation performance. |\n",
        "| **Interpretation** | Visualizing **Kernels** and **Feature Maps** to see what the \"black box\" is actually seeing. |\n",
        "\n",
        "### Next Steps\n",
        "To further your expertise, consider exploring:\n",
        "1.  **Distill.pub**: Read the article on \"Feature Visualization\" for deeper insights into model interpretation.\n",
        "2.  **Implement ResNets**: Try building a model with skip connections in Keras.\n",
        "3.  **Experiment with GANs**: Attempt to generate simple images (like digits) using adversarial training.\n",
        "\n",
        "Good luck with your Image Modeling journey!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}