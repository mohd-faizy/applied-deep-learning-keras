{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"  background: linear-gradient(145deg, #0f172a, #1e293b);  border: 4px solid transparent;  border-radius: 14px;  padding: 18px 22px;  margin: 12px 0;  font-size: 26px;  font-weight: 600;  color: #f8fafc;  box-shadow: 0 6px 14px rgba(0,0,0,0.25);  background-clip: padding-box;  position: relative;\">  <div style=\"    position: absolute;    inset: 0;    padding: 4px;    border-radius: 14px;    background: linear-gradient(90deg, #06b6d4, #3b82f6, #8b5cf6);    -webkit-mask:       linear-gradient(#fff 0 0) content-box,       linear-gradient(#fff 0 0);    -webkit-mask-composite: xor;    mask-composite: exclude;    pointer-events: none;  \"></div>    <b>INTRODUCTION TO CNNs: IMAGE MODELING WITH KERAS</b>    <br/>  <span style=\"color:#9ca3af; font-size: 18px; font-weight: 400;\">(Foundations of Image Processing and Classification)</span></div>\n",
        "\n",
        "## Table of Contents\n",
        "1. [Introduction & Prerequisites](#section-1)\n",
        "2. [Images as Data](#section-2)\n",
        "3. [Modifying Image Data](#section-3)\n",
        "4. [Introduction to Image Classification](#section-4)\n",
        "5. [Representing Class Data: One-Hot Encoding](#section-5)\n",
        "6. [Building a Classifier with Keras](#section-6)\n",
        "7. [Training and Evaluation](#section-7)\n",
        "8. [Conclusion](#section-8)\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 1. INTRODUCTION & PREREQUISITES</span><br>\n",
        "\n",
        "This notebook serves as a comprehensive introduction to Convolutional Neural Networks (CNNs) and image modeling using Keras. Before diving into the code, it is essential to understand the foundational concepts required to build effective image classifiers.\n",
        "\n",
        "### Prerequisites\n",
        "To successfully implement the concepts in this notebook, familiarity with the following Machine Learning topics is assumed:\n",
        "\n",
        "*   **Overfitting**: Understanding when a model learns the training data too well but fails to generalize to new data.\n",
        "*   **Model Evaluation**: Metrics used to assess performance (Accuracy, Loss).\n",
        "*   **Cross-validation**: Techniques to ensure the model is robust across different subsets of data.\n",
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> This notebook focuses on the transition from standard machine learning to Deep Learning for computer vision. We will start by treating images as numerical matrices. </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 2. IMAGES AS DATA</span><br>\n",
        "\n",
        "Computers \"see\" images as grids of numbers. In this section, we will load an image and inspect its numerical properties.\n",
        "\n",
        "### Loading and Visualizing Images\n",
        "We use `matplotlib.pyplot` to read and display images. An image is essentially a 3-dimensional array: height, width, and color channels (Red, Green, Blue).\n",
        "\n",
        "#### Original Code (From Slides)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Note: This code assumes a file named 'stop_sign.jpg' exists locally\n",
        "# data = plt.imread('stop_sign.jpg')\n",
        "# plt.imshow(data)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Enhanced Code (Executable)\n",
        "Since we do not have the local file, we will generate a sample image to demonstrate the concept.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create a synthetic image (Gradient) to simulate loaded data\n",
        "# Creating a 100x100 RGB image\n",
        "height, width = 100, 100\n",
        "data = np.zeros((height, width, 3))\n",
        "\n",
        "# Fill with some color gradients\n",
        "for y in range(height):\n",
        "    for x in range(width):\n",
        "        data[y, x] = [x/width, y/height, 0.5] # R, G, B values normalized 0-1\n",
        "\n",
        "plt.title(\"Synthetic Image Data\")\n",
        "plt.imshow(data)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Inspecting Image Structure\n",
        "Images are stored as NumPy arrays. The `.shape` attribute reveals the dimensions.\n",
        "\n",
        "*   **Structure**: `(Height, Width, Channels)`\n",
        "*   **Channels**: Usually 3 for RGB images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the shape of the image data\n",
        "print(f\"Image Shape: {data.shape}\")\n",
        "\n",
        "# Accessing a specific pixel\n",
        "# In the slides, data[1000, 1500] accessed a red pixel\n",
        "# Here we access pixel at row 50, column 50\n",
        "pixel_value = data[50, 50]\n",
        "print(f\"Pixel value at [50, 50]: {pixel_value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Explanation**:\n",
        "1.  **Shape**: The output `(100, 100, 3)` confirms it is a 100x100 pixel image with 3 color channels.\n",
        "2.  **Pixel Access**: `data[row, col]` returns an array of 3 values representing the intensity of Red, Green, and Blue at that specific point.\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 3. MODIFYING IMAGE DATA</span><br>\n",
        "\n",
        "Since images are just NumPy arrays, we can manipulate them using standard array slicing and assignment. This is useful for preprocessing, masking, or data augmentation.\n",
        "\n",
        "### Method 1: Modifying Color Channels\n",
        "We can isolate specific color channels. For example, setting the Green (index 1) and Blue (index 2) channels to 0 leaves only the Red component.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy to avoid modifying the original\n",
        "data_red = data.copy()\n",
        "\n",
        "# Set Green channel to 0\n",
        "data_red[:, :, 1] = 0\n",
        "\n",
        "# Set Blue channel to 0\n",
        "data_red[:, :, 2] = 0\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.title(\"Red Channel Only\")\n",
        "plt.imshow(data_red)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Method 2: Modifying Spatial Regions (Cropping/Masking)\n",
        "We can also modify specific rectangular regions of the image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy\n",
        "data_modified = data.copy()\n",
        "\n",
        "# Set a rectangular region to pure Green [0, 1, 0]\n",
        "# Rows 20 to 60, Columns 20 to 60\n",
        "data_modified[20:60, 20:60, :] = [0, 1, 0]\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.title(\"Region Modified to Green\")\n",
        "plt.imshow(data_modified)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> In the context of the Fashion MNIST dataset (black and white images), modifying pixels is often used to normalize data or add noise for training robustness. </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 4. INTRODUCTION TO IMAGE CLASSIFICATION</span><br>\n",
        "\n",
        "Image classification is the process of assigning a label to an image based on its visual content.\n",
        "\n",
        "### The Workflow\n",
        "1.  **Input**: An image (e.g., a picture of a shoe).\n",
        "2.  **Classifier**: A mathematical function (Neural Network) that processes the pixel data.\n",
        "3.  **Output**: A predicted label (e.g., \"shoe\").\n",
        "\n",
        "### Training vs. Evaluation\n",
        "*   **Training**: The classifier sees an image (e.g., a dress) and is told the correct label (\"dress\"). It adjusts its internal parameters to minimize the error.\n",
        "*   **Evaluation**: The classifier sees a new image it hasn't seen before. It predicts a label. We compare this prediction to the actual label to calculate accuracy.\n",
        "\n",
        "**Example Scenarios**:\n",
        "*   Input: Image of a shoe $\\rightarrow$ Classifier $\\rightarrow$ Prediction: \"shoe\" (Correct)\n",
        "*   Input: Image of a t-shirt $\\rightarrow$ Classifier $\\rightarrow$ Prediction: \"shoe\" (Incorrect)\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 5. REPRESENTING CLASS DATA: ONE-HOT ENCODING</span><br>\n",
        "\n",
        "Neural networks work with numbers, not strings. We cannot feed the string \"shoe\" into the network as a target. Instead, we use **One-Hot Encoding**.\n",
        "\n",
        "### Concept\n",
        "If we have 3 categories: `[\"t-shirt\", \"dress\", \"shoe\"]`, we represent them as vectors of length 3.\n",
        "\n",
        "| Category | Index | One-Hot Vector |\n",
        "| :--- | :--- | :--- |\n",
        "| t-shirt | 0 | `[1, 0, 0]` |\n",
        "| dress | 1 | `[0, 1, 0]` |\n",
        "| shoe | 2 | `[0, 0, 1]` |\n",
        "\n",
        "### Manual Implementation\n",
        "Let's implement the logic found in the slides to convert a list of string labels into a one-hot encoded matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. The raw labels\n",
        "labels = [\"shoe\", \"dress\", \"shoe\", \"t-shirt\", \"shoe\", \"t-shirt\", \"shoe\", \"dress\"]\n",
        "\n",
        "# 2. Define categories\n",
        "categories = np.array([\"t-shirt\", \"dress\", \"shoe\"])\n",
        "n_categories = 3\n",
        "\n",
        "# 3. Initialize zero matrix\n",
        "ohe_labels = np.zeros((len(labels), n_categories))\n",
        "\n",
        "# 4. Fill the matrix\n",
        "for ii in range(len(labels)):\n",
        "    # Find the index where the category matches the current label\n",
        "    jj = np.where(categories == labels[ii])\n",
        "    # Set that index to 1\n",
        "    ohe_labels[ii, jj] = 1\n",
        "\n",
        "print(\"One-Hot Encoded Labels:\")\n",
        "print(ohe_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Testing Predictions\n",
        "We can verify predictions mathematically. If a model outputs a prediction vector, we can compare it to the ground truth using element-wise multiplication.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ground Truth (Test)\n",
        "test = np.array([\n",
        "    [0., 0., 1.], # shoe\n",
        "    [0., 1., 0.], # dress\n",
        "    [0., 0., 1.], # shoe\n",
        "    [0., 1., 0.], # dress (Note: Slide example had variations, using standard logic here)\n",
        "])\n",
        "\n",
        "# Model Predictions (Hypothetical)\n",
        "prediction = np.array([\n",
        "    [0., 0., 1.], # Correct\n",
        "    [0., 1., 0.], # Correct\n",
        "    [0., 0., 1.], # Correct\n",
        "    [1., 0., 0.], # Incorrect (Predicted t-shirt, was dress)\n",
        "])\n",
        "\n",
        "# Calculate correct predictions\n",
        "# Element-wise multiplication: 1*1 = 1 (correct), 1*0 = 0 (incorrect)\n",
        "correct_count = (test * prediction).sum()\n",
        "\n",
        "print(f\"Number of correct predictions: {correct_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 6. BUILDING A CLASSIFIER WITH KERAS</span><br>\n",
        "\n",
        "We will now build a neural network using the Keras `Sequential` API. We will simulate the \"Fashion MNIST\" dataset structure used in the presentation.\n",
        "\n",
        "### 1. Data Preparation\n",
        "The input images are 28x28 pixels. Standard dense neural networks require flat vectors, not 2D grids. We must reshape the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulating Data for 50 samples of 28x28 images\n",
        "# In a real scenario: (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "train_data_raw = np.random.random((50, 28, 28, 1))\n",
        "\n",
        "print(f\"Original Shape: {train_data_raw.shape}\")\n",
        "\n",
        "# Flattening the data: 28 * 28 = 784\n",
        "train_data = train_data_raw.reshape((50, 784))\n",
        "\n",
        "print(f\"Flattened Shape: {train_data.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 2. Defining the Model Architecture\n",
        "We use a `Sequential` model with `Dense` (fully connected) layers.\n",
        "\n",
        "*   **Input Layer**: Accepts the flattened vector (784 features).\n",
        "*   **Hidden Layers**: Use `relu` activation to learn non-linear patterns.\n",
        "*   **Output Layer**: Uses `softmax` activation to output probabilities for the 3 classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Initialize model\n",
        "model = Sequential()\n",
        "\n",
        "# First hidden layer: 10 neurons, ReLU activation, input shape required for first layer\n",
        "model.add(Dense(10, activation='relu', input_shape=(784,)))\n",
        "\n",
        "# Second hidden layer: 10 neurons, ReLU activation\n",
        "model.add(Dense(10, activation='relu'))\n",
        "\n",
        "# Output layer: 3 neurons (for 3 classes), Softmax activation\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div style=\"background: #e0f2fe; border-left: 16px solid #0284c7; padding: 14px 18px; border-radius: 8px; font-size: 18px; color: #075985;\"> ðŸ’¡ <b>Tip:</b> The output layer size <b>must</b> match the number of categories in your one-hot encoded labels. Softmax ensures the outputs sum to 1, representing probabilities. </div>\n",
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 7. TRAINING AND EVALUATION</span><br>\n",
        "\n",
        "Once the architecture is defined, we must compile the model with an optimizer and loss function, then train it on data.\n",
        "\n",
        "### 1. Compiling the Model\n",
        "*   **Optimizer**: `adam` (Adaptive Moment Estimation) - a standard, efficient optimizer.\n",
        "*   **Loss**: `categorical_crossentropy` - used for multi-class classification with one-hot encoding.\n",
        "*   **Metrics**: `['accuracy']` - to monitor performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 2. Training (Fitting) the Model\n",
        "We fit the model to the training data.\n",
        "*   **Validation Split**: 0.2 means 20% of the data is held back to check generalization during training.\n",
        "*   **Epochs**: Number of times the model sees the entire dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generating dummy labels for the 50 samples (3 classes)\n",
        "# In reality, these would be your actual one-hot encoded labels\n",
        "train_labels = np.zeros((50, 3))\n",
        "for i in range(50):\n",
        "    train_labels[i, np.random.randint(0, 3)] = 1\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_data, train_labels,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=3,\n",
        "                    verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 3. Evaluating on Test Data\n",
        "Finally, we evaluate the model on a separate test set to see how well it performs on unseen data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate test data (10 samples)\n",
        "test_data_raw = np.random.random((10, 28, 28, 1))\n",
        "test_data = test_data_raw.reshape((10, 784))\n",
        "\n",
        "# Simulate test labels\n",
        "test_labels = np.zeros((10, 3))\n",
        "for i in range(10):\n",
        "    test_labels[i, np.random.randint(0, 3)] = 1\n",
        "\n",
        "# Evaluate\n",
        "loss, accuracy = model.evaluate(test_data, test_labels)\n",
        "\n",
        "print(f\"\\nTest Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "<br><span style=\"  display: inline-block;  color: #fff;  background: linear-gradient(135deg, #a31616ff, #02b7ffff);  padding: 12px 20px;  border-radius: 12px;  font-size: 28px;  font-weight: 700;  box-shadow: 0 4px 12px rgba(0,0,0,0.2);  transition: transform 0.2s ease, box-shadow 0.2s ease;\">  ðŸ§¾ 8. CONCLUSION</span><br>\n",
        "\n",
        "In this notebook, we covered the fundamental steps to build an image classifier using Keras:\n",
        "\n",
        "1.  **Images as Data**: We learned that images are simply 3D NumPy arrays (Height, Width, Channels) and can be manipulated mathematically.\n",
        "2.  **Preprocessing**: We explored how to modify pixel values and reshape 2D images into 1D vectors for dense layers.\n",
        "3.  **Encoding**: We implemented One-Hot Encoding to convert categorical text labels into machine-readable vectors.\n",
        "4.  **Modeling**: We built a `Sequential` model with `Dense` layers, using `relu` for hidden layers and `softmax` for the output.\n",
        "5.  **Training**: We compiled the model with `categorical_crossentropy` and trained it using `model.fit()`.\n",
        "\n",
        "**Next Steps**:\n",
        "*   Experiment with more layers or neurons to see if accuracy improves.\n",
        "*   Try using `Conv2D` layers (Convolutional layers) instead of just `Dense` layers to better capture spatial hierarchies in images.\n",
        "*   Apply these techniques to the full Fashion-MNIST dataset available in `tensorflow.keras.datasets`.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}